{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bita6e133adcd1e45d886e3cdf5747d7530",
   "display_name": "Python 3.6.8 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# 导入相关地依赖库\n",
    "import matplotlib.pyplot as  plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, MaxPool2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os,cv2,random,zipfile,fitz,re,shutil,time\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # 显示进度条\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "EXCEL_Data=\"C:/Users/ASUS/Desktop/problem c/mydata3.xlsx\"\n",
    "mydata2=pd.read_excel(EXCEL_Data,engine='openpyxl')#用openpyxl代替xlrd，不然不支持xlsx文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义数据集的大小、路劲等参数\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "Img_Data_Path = \"./2021MCM_ProblemC_Files\"\n",
    "EXCEL_Data2=\"./2021_MCM_Problem_C_Data/2021MCM_ProblemC_ Images_by_GlobalID.xlsx\"\n",
    "\n",
    "CATEGORIES = ['Positive ID', 'Negative ID']\n",
    "image_size = 128\n",
    "EPOCH=10\n",
    "NUM_EPOCH=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取出文件里边的数据\n",
    "DataOfName=mydata2['FileName'].values.tolist()\n",
    "DataOfLab=mydata2['Lab Status'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "run:3.7159403s\n",
      "Total take 1 imges\n",
      "run:0.4279706000000001s\n",
      "Total take 1 imges\n",
      "run:0.7243175000000015s\n",
      "Total take 1 imges\n",
      "run:1.3569624000000005s\n",
      "Total take 2 imges\n",
      "run:2.0960205000000016s\n",
      "Total take 3 imges\n",
      "run:2.6734530999999997s\n",
      "Total take 4 imges\n",
      "run:3.412885000000003s\n",
      "Total take 5 imges\n",
      "run:4.011683400000003s\n",
      "Total take 6 imges\n"
     ]
    }
   ],
   "source": [
    "#得到全部的数据\n",
    "Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data=create_data(Img_Data_Path,DataOfName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TRAIN=50\n",
    "EPOCH=10\n",
    "ACC=[]\n",
    "VAL_ACC=[]\n",
    "LOSS=[]\n",
    "VAL_LOSS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 搭建卷积神经网络\n",
    "model = Sequential()\n",
    "\n",
    "# 第一层卷积，16个3*3的卷积核\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(image_size,image_size,3)))\n",
    "model.add(MaxPool2D(pool_size=(3,3),strides=(2,2)))\n",
    "model.add(Dropout(0.25))       # 随机丢弃\n",
    "\n",
    "# 第二层卷积\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 1), activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(1, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=(1, 1), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(3,3),strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1, 1), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(3,3),strides=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# 拉伸层，在全连接层前得进行拉伸操作\n",
    "model.add(Flatten())\n",
    "\n",
    "# 全连接层\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=2, activation = 'softmax'))\n",
    "\n",
    "\n",
    "# 进行模型设置\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "#  设置生成器得参数\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model04_{epoch:02d}_{val_loss:.2f}.h5', # 模型保存地址，默认保存整个模型save_weights_only=False\n",
    "        save_best_only=True,         # 只保存在验证集上性能最好的模型\n",
    "        monitor='val_loss',          # 设置被检测的数据val_acc或者val_loss，测试集的准确率和损失率\n",
    "        verbose=1                    # 1打印详细信息\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_OF_TRAIN=50\n",
    "EPOCH=10\n",
    "ACC=[]\n",
    "VAL_ACC=[]\n",
    "LOSS=[]\n",
    "VAL_LOSS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TRAIN=10\n",
    "EPOCH=25\n",
    "ACC=[]\n",
    "VAL_ACC=[]\n",
    "LOSS=[]\n",
    "VAL_LOSS=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6915 - acc: 0.5882\n",
      "Epoch 00001: val_loss improved from inf to 0.68918, saving model to model04_01_0.69.h5\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.6915 - acc: 0.5882 - val_loss: 0.6892 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6792 - acc: 0.5294\n",
      "Epoch 00002: val_loss did not improve from 0.68918\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6792 - acc: 0.5294 - val_loss: 0.6963 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6867 - acc: 0.5294\n",
      "Epoch 00003: val_loss did not improve from 0.68918\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6867 - acc: 0.5294 - val_loss: 0.6988 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7628 - acc: 0.4706\n",
      "Epoch 00004: val_loss did not improve from 0.68918\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7628 - acc: 0.4706 - val_loss: 0.6892 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7243 - acc: 0.3529\n",
      "Epoch 00005: val_loss improved from 0.68918 to 0.68884, saving model to model04_05_0.69.h5\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7243 - acc: 0.3529 - val_loss: 0.6888 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6050 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6050 - acc: 0.5882 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6703 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6703 - acc: 0.7059 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6839 - acc: 0.4706\n",
      "Epoch 00008: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6839 - acc: 0.4706 - val_loss: 0.6918 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6928 - acc: 0.5882\n",
      "Epoch 00009: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6928 - acc: 0.5882 - val_loss: 0.6899 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7293 - acc: 0.2941\n",
      "Epoch 00010: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7293 - acc: 0.2941 - val_loss: 0.6895 - val_acc: 0.6667\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6820 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6820 - acc: 0.5294 - val_loss: 0.6961 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6638 - acc: 0.4706\n",
      "Epoch 00002: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6638 - acc: 0.4706 - val_loss: 0.6964 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.4706\n",
      "Epoch 00003: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6936 - acc: 0.4706 - val_loss: 0.6986 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6171 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6171 - acc: 0.7059 - val_loss: 0.7013 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6371 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6371 - acc: 0.7059 - val_loss: 0.7036 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6240 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6240 - acc: 0.7059 - val_loss: 0.7024 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5750 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5750 - acc: 0.7647 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6436 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6436 - acc: 0.5882 - val_loss: 0.7058 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7256 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7256 - acc: 0.5294 - val_loss: 0.7150 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4785 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4785 - acc: 0.7647 - val_loss: 0.7296 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4813 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4813 - acc: 0.7059 - val_loss: 0.7105 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6757 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6757 - acc: 0.7059 - val_loss: 0.7065 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6485 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6485 - acc: 0.5882 - val_loss: 0.7026 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6975 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6975 - acc: 0.5882 - val_loss: 0.7125 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5672 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5672 - acc: 0.5882 - val_loss: 0.7389 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6923 - acc: 0.6471 - val_loss: 0.7170 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5166 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5166 - acc: 0.7059 - val_loss: 0.7069 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4172 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4172 - acc: 0.8235 - val_loss: 0.7071 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4745 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4745 - acc: 0.7647 - val_loss: 0.7070 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4188 - acc: 0.9412\n",
      "Epoch 00010: val_loss did not improve from 0.68884\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4188 - acc: 0.9412 - val_loss: 0.7085 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5021 - acc: 0.8235\n",
      "Epoch 00001: val_loss improved from 0.68884 to 0.61483, saving model to model04_01_0.61.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.5021 - acc: 0.8235 - val_loss: 0.6148 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5858 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.61483\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5858 - acc: 0.7059 - val_loss: 0.6352 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5296 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.61483\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5296 - acc: 0.7647 - val_loss: 0.6361 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6417 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.61483\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6417 - acc: 0.7647 - val_loss: 0.6249 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5075 - acc: 0.7059\n",
      "Epoch 00005: val_loss improved from 0.61483 to 0.59365, saving model to model04_05_0.59.h5\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5075 - acc: 0.7059 - val_loss: 0.5936 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7384 - acc: 0.4118\n",
      "Epoch 00006: val_loss did not improve from 0.59365\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7384 - acc: 0.4118 - val_loss: 0.6367 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5476 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.59365\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5476 - acc: 0.6471 - val_loss: 0.6419 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5511 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.59365\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5511 - acc: 0.7059 - val_loss: 0.6205 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5085 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.59365\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5085 - acc: 0.8235 - val_loss: 0.5944 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5046 - acc: 0.7059\n",
      "Epoch 00010: val_loss improved from 0.59365 to 0.58689, saving model to model04_10_0.59.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.5046 - acc: 0.7059 - val_loss: 0.5869 - val_acc: 0.6667\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5758 - acc: 0.8824\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5758 - acc: 0.8824 - val_loss: 0.7497 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5779 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5779 - acc: 0.7647 - val_loss: 0.7793 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4987 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4987 - acc: 0.7647 - val_loss: 0.7856 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5119 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5119 - acc: 0.8235 - val_loss: 0.7712 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4791 - acc: 0.8235\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4791 - acc: 0.8235 - val_loss: 0.7708 - val_acc: 0.3333\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3971 - acc: 0.8824\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3971 - acc: 0.8824 - val_loss: 0.7567 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5179 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5179 - acc: 0.7059 - val_loss: 0.7959 - val_acc: 0.3333\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4770 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4770 - acc: 0.8235 - val_loss: 0.8683 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4572 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.4572 - acc: 0.6471 - val_loss: 0.8645 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4839 - acc: 0.8824\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4839 - acc: 0.8824 - val_loss: 0.8298 - val_acc: 0.3333\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8004 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.8004 - acc: 0.5882 - val_loss: 0.6812 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7868 - acc: 0.4706\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7868 - acc: 0.4706 - val_loss: 0.6863 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7752 - acc: 0.5294\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7752 - acc: 0.5294 - val_loss: 0.6874 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6295 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6295 - acc: 0.5882 - val_loss: 0.6917 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6503 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6503 - acc: 0.7059 - val_loss: 0.6971 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6386 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6386 - acc: 0.6471 - val_loss: 0.7004 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5990 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5990 - acc: 0.7059 - val_loss: 0.7016 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6108 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6108 - acc: 0.6471 - val_loss: 0.7020 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6007 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6007 - acc: 0.7647 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5397 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5397 - acc: 0.7059 - val_loss: 0.7026 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5807 - acc: 0.7647\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5807 - acc: 0.7647 - val_loss: 0.7016 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5846 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5846 - acc: 0.7647 - val_loss: 0.7020 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5802 - acc: 0.8235\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5802 - acc: 0.8235 - val_loss: 0.7020 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6049 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6049 - acc: 0.7059 - val_loss: 0.6975 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5362 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5362 - acc: 0.7647 - val_loss: 0.6847 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5007 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5007 - acc: 0.8235 - val_loss: 0.6679 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5885 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5885 - acc: 0.7647 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5364 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5364 - acc: 0.7647 - val_loss: 0.7155 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4974 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4974 - acc: 0.7059 - val_loss: 0.7227 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6022 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6022 - acc: 0.6471 - val_loss: 0.7274 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4818 - acc: 0.8235\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4818 - acc: 0.8235 - val_loss: 0.7499 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4835 - acc: 0.8235\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4835 - acc: 0.8235 - val_loss: 0.7581 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4588 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4588 - acc: 0.7647 - val_loss: 0.7652 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5867 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5867 - acc: 0.7059 - val_loss: 0.7835 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4583 - acc: 0.8235\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4583 - acc: 0.8235 - val_loss: 0.8032 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5099 - acc: 0.7647\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.5099 - acc: 0.7647 - val_loss: 0.8216 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5034 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5034 - acc: 0.7647 - val_loss: 0.8296 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4659 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4659 - acc: 0.7059 - val_loss: 0.8325 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4307 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4307 - acc: 0.7647 - val_loss: 0.8281 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4633 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4633 - acc: 0.7059 - val_loss: 0.8474 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9772 - acc: 0.4706\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9772 - acc: 0.4706 - val_loss: 0.8523 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7695 - acc: 0.4706\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7695 - acc: 0.4706 - val_loss: 0.8830 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8598 - acc: 0.5294\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8598 - acc: 0.5294 - val_loss: 0.8578 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9136 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.9136 - acc: 0.5294 - val_loss: 0.8214 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8081 - acc: 0.5294\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.8081 - acc: 0.5294 - val_loss: 0.7878 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7859 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7859 - acc: 0.5294 - val_loss: 0.7612 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7478 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7478 - acc: 0.5294 - val_loss: 0.7421 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7143 - acc: 0.5294\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7143 - acc: 0.5294 - val_loss: 0.7296 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6986 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6986 - acc: 0.5294 - val_loss: 0.7220 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6896 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6896 - acc: 0.5294 - val_loss: 0.7175 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6604 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.6604 - acc: 0.5882 - val_loss: 0.7044 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6881 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6881 - acc: 0.5882 - val_loss: 0.7010 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6476 - acc: 0.8824\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6476 - acc: 0.8824 - val_loss: 0.6987 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6464 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6464 - acc: 0.7059 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6570 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6570 - acc: 0.5882 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6610 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6610 - acc: 0.5882 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6559 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6559 - acc: 0.6471 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6598 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6598 - acc: 0.5882 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6295 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6295 - acc: 0.6471 - val_loss: 0.6916 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6062 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6062 - acc: 0.6471 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7253 - acc: 0.4118\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.7253 - acc: 0.4118 - val_loss: 0.6976 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6661 - acc: 0.4706\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6661 - acc: 0.4706 - val_loss: 0.6996 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6955 - acc: 0.4706\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6955 - acc: 0.4706 - val_loss: 0.7002 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6736 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6736 - acc: 0.5882 - val_loss: 0.7008 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6715 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6715 - acc: 0.5882 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6789 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6789 - acc: 0.5882 - val_loss: 0.7014 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6941 - acc: 0.5294 - val_loss: 0.7017 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6909 - acc: 0.5294\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6909 - acc: 0.5294 - val_loss: 0.7019 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6904 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6904 - acc: 0.5294 - val_loss: 0.7020 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6903 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6903 - acc: 0.5294 - val_loss: 0.7022 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6942 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.6942 - acc: 0.5294 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6954 - acc: 0.5294\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6954 - acc: 0.5294 - val_loss: 0.7024 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6929 - acc: 0.5294\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6929 - acc: 0.5294 - val_loss: 0.7024 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6904 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6904 - acc: 0.5294 - val_loss: 0.7024 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.5294\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6938 - acc: 0.5294 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6916 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6916 - acc: 0.5294 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6921 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6921 - acc: 0.5294 - val_loss: 0.7022 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5294\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6941 - acc: 0.5294 - val_loss: 0.7021 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6950 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6950 - acc: 0.5294 - val_loss: 0.7019 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6923 - acc: 0.5294 - val_loss: 0.7017 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6886 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6886 - acc: 0.5294 - val_loss: 0.7014 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6865 - acc: 0.5294\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6865 - acc: 0.5294 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6793 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6793 - acc: 0.5882 - val_loss: 0.7008 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6782 - acc: 0.5294 - val_loss: 0.7004 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6726 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6726 - acc: 0.5882 - val_loss: 0.7001 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6686 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6686 - acc: 0.5294 - val_loss: 0.6995 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6158 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6158 - acc: 0.7647 - val_loss: 0.6969 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6173 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6173 - acc: 0.6471 - val_loss: 0.6907 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6441 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6441 - acc: 0.7059 - val_loss: 0.6829 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5922 - acc: 0.5294 - val_loss: 0.6764 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6501 - acc: 0.4706\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6501 - acc: 0.4706 - val_loss: 0.6383 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7517 - acc: 0.4706\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7517 - acc: 0.4706 - val_loss: 0.6557 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6747 - acc: 0.5294\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6747 - acc: 0.5294 - val_loss: 0.6789 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6659 - acc: 0.6471\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6659 - acc: 0.6471 - val_loss: 0.6919 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6585 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6585 - acc: 0.7059 - val_loss: 0.6988 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6848 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6848 - acc: 0.5882 - val_loss: 0.7029 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6991 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6991 - acc: 0.5294 - val_loss: 0.7047 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6588 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6588 - acc: 0.6471 - val_loss: 0.7049 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6918 - acc: 0.5882\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6918 - acc: 0.5882 - val_loss: 0.7042 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7020 - acc: 0.5882\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7020 - acc: 0.5882 - val_loss: 0.7029 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6967 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6967 - acc: 0.5294 - val_loss: 0.7008 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6847 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6847 - acc: 0.5882 - val_loss: 0.6990 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6835 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6835 - acc: 0.6471 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6291 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6291 - acc: 0.8235 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6583 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6583 - acc: 0.7059 - val_loss: 0.6883 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6497 - acc: 0.7647\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6497 - acc: 0.7647 - val_loss: 0.6815 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6170 - acc: 0.8824\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6170 - acc: 0.8824 - val_loss: 0.6711 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6048 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6048 - acc: 0.6471 - val_loss: 0.6588 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7343 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7343 - acc: 0.7059 - val_loss: 0.6579 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7047 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.7047 - acc: 0.7647 - val_loss: 0.6636 - val_acc: 0.6667\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7153 - acc: 0.6471\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.7153 - acc: 0.6471 - val_loss: 0.6845 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6728 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6728 - acc: 0.7059 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6657 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6657 - acc: 0.6471 - val_loss: 0.7025 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6433 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6433 - acc: 0.7647 - val_loss: 0.7051 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6835 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6835 - acc: 0.5882 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6733 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6733 - acc: 0.5294 - val_loss: 0.7069 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6810 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6810 - acc: 0.5294 - val_loss: 0.7075 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6766 - acc: 0.5294\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6766 - acc: 0.5294 - val_loss: 0.7082 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6930 - acc: 0.5294 - val_loss: 0.7087 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6805 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6805 - acc: 0.5294 - val_loss: 0.7090 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6585 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6585 - acc: 0.5294 - val_loss: 0.7124 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6490 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6490 - acc: 0.5882 - val_loss: 0.7130 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6614 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6614 - acc: 0.5882 - val_loss: 0.7127 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6654 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6654 - acc: 0.5882 - val_loss: 0.7117 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6943 - acc: 0.6471 - val_loss: 0.7097 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6282 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6282 - acc: 0.7059 - val_loss: 0.7061 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6222 - acc: 0.8235\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6222 - acc: 0.8235 - val_loss: 0.7008 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6369 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6369 - acc: 0.7647 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6617 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6617 - acc: 0.6471 - val_loss: 0.6865 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6948 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6948 - acc: 0.5294 - val_loss: 0.6846 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5988 - acc: 0.8235\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5988 - acc: 0.8235 - val_loss: 0.6905 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5256 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5256 - acc: 0.7647 - val_loss: 0.6913 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5624 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5624 - acc: 0.7647 - val_loss: 0.6949 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5936 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5936 - acc: 0.7059 - val_loss: 0.6991 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4641 - acc: 0.9412\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4641 - acc: 0.9412 - val_loss: 0.7024 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5080 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5080 - acc: 0.8235 - val_loss: 0.7026 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4847 - acc: 0.8235\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4847 - acc: 0.8235 - val_loss: 0.7035 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6778 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6778 - acc: 0.7647 - val_loss: 0.7101 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5693 - acc: 0.8824\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5693 - acc: 0.8824 - val_loss: 0.7182 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5054 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5054 - acc: 0.7647 - val_loss: 0.7256 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7350 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.7350 - acc: 0.7059 - val_loss: 0.5919 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7477 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.7477 - acc: 0.7059 - val_loss: 0.6046 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6314 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6314 - acc: 0.7059 - val_loss: 0.6338 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6441 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6441 - acc: 0.7059 - val_loss: 0.6581 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7353 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7353 - acc: 0.5882 - val_loss: 0.6684 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7361 - acc: 0.4706\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7361 - acc: 0.4706 - val_loss: 0.6719 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5954 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5954 - acc: 0.7647 - val_loss: 0.6691 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6064 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6064 - acc: 0.7059 - val_loss: 0.6600 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6162 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6162 - acc: 0.7059 - val_loss: 0.6562 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7455 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7455 - acc: 0.5294 - val_loss: 0.6630 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7146 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.7146 - acc: 0.7059 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6607 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6607 - acc: 0.6471 - val_loss: 0.7003 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6365 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6365 - acc: 0.7059 - val_loss: 0.7039 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6761 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6761 - acc: 0.5294 - val_loss: 0.7063 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6744 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6744 - acc: 0.5882 - val_loss: 0.7074 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6605 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6605 - acc: 0.5294 - val_loss: 0.7074 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6619 - acc: 0.5882\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6619 - acc: 0.5882 - val_loss: 0.7070 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6688 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6688 - acc: 0.5882 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6689 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6689 - acc: 0.5294 - val_loss: 0.7049 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6637 - acc: 0.5882\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6637 - acc: 0.5882 - val_loss: 0.7029 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6345 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6345 - acc: 0.5882 - val_loss: 0.7055 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6799 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6799 - acc: 0.5882 - val_loss: 0.7048 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6485 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6485 - acc: 0.6471 - val_loss: 0.7033 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6120 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6120 - acc: 0.7647 - val_loss: 0.7013 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6713 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6713 - acc: 0.5882 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6293 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6293 - acc: 0.5294 - val_loss: 0.6992 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6535 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6535 - acc: 0.6471 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6199 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6199 - acc: 0.6471 - val_loss: 0.7027 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6049 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6049 - acc: 0.7647 - val_loss: 0.7041 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6193 - acc: 0.5882\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6193 - acc: 0.5882 - val_loss: 0.7053 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6439 - acc: 0.6471\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6439 - acc: 0.6471 - val_loss: 0.7221 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6042 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6042 - acc: 0.7059 - val_loss: 0.7259 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5981 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5981 - acc: 0.7647 - val_loss: 0.7308 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5869 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5869 - acc: 0.8235 - val_loss: 0.7348 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6313 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6313 - acc: 0.6471 - val_loss: 0.7400 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5588 - acc: 0.7647\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5588 - acc: 0.7647 - val_loss: 0.7487 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5537 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5537 - acc: 0.7647 - val_loss: 0.7595 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6358 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6358 - acc: 0.6471 - val_loss: 0.7732 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5742 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5742 - acc: 0.7647 - val_loss: 0.7928 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6250 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6250 - acc: 0.6471 - val_loss: 0.8094 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5201 - acc: 0.8235\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5201 - acc: 0.8235 - val_loss: 0.7568 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6099 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6099 - acc: 0.5882 - val_loss: 0.7646 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4842 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4842 - acc: 0.7647 - val_loss: 0.7413 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4854 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4854 - acc: 0.8235 - val_loss: 0.7260 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5055 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5055 - acc: 0.7647 - val_loss: 0.8063 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5797 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5797 - acc: 0.7059 - val_loss: 0.8352 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6857 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6857 - acc: 0.5294 - val_loss: 0.8080 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5310 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5310 - acc: 0.7647 - val_loss: 0.7426 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5249 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5249 - acc: 0.7647 - val_loss: 0.7271 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5958 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5958 - acc: 0.7059 - val_loss: 0.7979 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8082 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8082 - acc: 0.5882 - val_loss: 1.0247 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4782 - acc: 0.8824\n",
      "Epoch 00002: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4782 - acc: 0.8824 - val_loss: 1.0343 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7534 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7534 - acc: 0.5882 - val_loss: 0.9858 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7476 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7476 - acc: 0.5294 - val_loss: 0.9139 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6260 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6260 - acc: 0.6471 - val_loss: 0.8570 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6616 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6616 - acc: 0.5882 - val_loss: 0.8158 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4978 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4978 - acc: 0.7647 - val_loss: 0.7931 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5683 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5683 - acc: 0.7647 - val_loss: 0.7695 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4599 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4599 - acc: 0.8235 - val_loss: 0.7514 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5442 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.58689\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5442 - acc: 0.8235 - val_loss: 0.7473 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6943 - acc: 0.5882\n",
      "Epoch 00001: val_loss improved from 0.58689 to 0.55683, saving model to model04_01_0.56.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.6943 - acc: 0.5882 - val_loss: 0.5568 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6548 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6548 - acc: 0.7059 - val_loss: 0.5673 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8491 - acc: 0.4118\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8491 - acc: 0.4118 - val_loss: 0.5913 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6846 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6846 - acc: 0.5294 - val_loss: 0.6117 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6190 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6190 - acc: 0.7647 - val_loss: 0.6284 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6902 - acc: 0.4706\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6902 - acc: 0.4706 - val_loss: 0.6499 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6131 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6131 - acc: 0.6471 - val_loss: 0.6658 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6103 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6103 - acc: 0.6471 - val_loss: 0.6755 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6363 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6363 - acc: 0.7059 - val_loss: 0.6827 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6882 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6882 - acc: 0.7059 - val_loss: 0.6875 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5527 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5527 - acc: 0.7059 - val_loss: 0.7182 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6168 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6168 - acc: 0.6471 - val_loss: 0.7180 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5933 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5933 - acc: 0.7647 - val_loss: 0.7174 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5840 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5840 - acc: 0.7059 - val_loss: 0.7166 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5401 - acc: 0.8824\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5401 - acc: 0.8824 - val_loss: 0.7151 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5530 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5530 - acc: 0.8235 - val_loss: 0.7127 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5401 - acc: 0.8235\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5401 - acc: 0.8235 - val_loss: 0.7116 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5628 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5628 - acc: 0.5882 - val_loss: 0.7122 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6406 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6406 - acc: 0.7059 - val_loss: 0.7150 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5289 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5289 - acc: 0.7647 - val_loss: 0.7183 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5293 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5293 - acc: 0.7059 - val_loss: 0.6570 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7333 - acc: 0.4118\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7333 - acc: 0.4118 - val_loss: 0.6590 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6140 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6140 - acc: 0.7647 - val_loss: 0.6618 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6893 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6893 - acc: 0.7059 - val_loss: 0.6615 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4938 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.4938 - acc: 0.7647 - val_loss: 0.6541 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6279 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6279 - acc: 0.7059 - val_loss: 0.6463 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4706 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4706 - acc: 0.7647 - val_loss: 0.6378 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7096 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7096 - acc: 0.5882 - val_loss: 0.6388 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8543 - acc: 0.4706\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.8543 - acc: 0.4706 - val_loss: 0.6517 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7045 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7045 - acc: 0.7059 - val_loss: 0.6777 - val_acc: 0.6667\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5150 - acc: 0.7647\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5150 - acc: 0.7647 - val_loss: 0.7093 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6027 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6027 - acc: 0.7647 - val_loss: 0.7148 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5639 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5639 - acc: 0.7059 - val_loss: 0.7173 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5861 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5861 - acc: 0.5882 - val_loss: 0.7153 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5632 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5632 - acc: 0.7059 - val_loss: 0.7101 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5641 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5641 - acc: 0.8235 - val_loss: 0.7023 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5835 - acc: 0.5882\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5835 - acc: 0.5882 - val_loss: 0.6973 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6251 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6251 - acc: 0.7059 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5286 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5286 - acc: 0.7059 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4635 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4635 - acc: 0.8235 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6879 - acc: 0.6471\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6879 - acc: 0.6471 - val_loss: 0.7191 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5959 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5959 - acc: 0.7059 - val_loss: 0.7278 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5630 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5630 - acc: 0.7647 - val_loss: 0.7327 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5788 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5788 - acc: 0.7059 - val_loss: 0.7359 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5374 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5374 - acc: 0.7647 - val_loss: 0.7375 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5918 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5918 - acc: 0.7059 - val_loss: 0.7384 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6281 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6281 - acc: 0.6471 - val_loss: 0.7386 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5298 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5298 - acc: 0.8235 - val_loss: 0.7358 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5616 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5616 - acc: 0.7647 - val_loss: 0.7289 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4869 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4869 - acc: 0.8235 - val_loss: 0.7201 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3723 - acc: 0.8824\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3723 - acc: 0.8824 - val_loss: 0.7195 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4239 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4239 - acc: 0.7647 - val_loss: 0.7178 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3720 - acc: 0.8824\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3720 - acc: 0.8824 - val_loss: 0.7137 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4316 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4316 - acc: 0.8235 - val_loss: 0.7209 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4479 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4479 - acc: 0.7059 - val_loss: 0.7489 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4579 - acc: 0.7647\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4579 - acc: 0.7647 - val_loss: 0.7722 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4755 - acc: 0.8235\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4755 - acc: 0.8235 - val_loss: 0.7901 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3888 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.3888 - acc: 0.8235 - val_loss: 0.7996 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3913 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3913 - acc: 0.8235 - val_loss: 0.8036 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5129 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5129 - acc: 0.7059 - val_loss: 0.7859 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6379 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6379 - acc: 0.7059 - val_loss: 0.6224 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4106 - acc: 0.8235\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4106 - acc: 0.8235 - val_loss: 0.6251 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7406 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7406 - acc: 0.7059 - val_loss: 0.6652 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5490 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5490 - acc: 0.7647 - val_loss: 0.6999 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5250 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5250 - acc: 0.7647 - val_loss: 0.7211 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5242 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5242 - acc: 0.6471 - val_loss: 0.7371 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5396 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5396 - acc: 0.7647 - val_loss: 0.7443 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6519 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6519 - acc: 0.6471 - val_loss: 0.7479 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6221 - acc: 0.5882\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6221 - acc: 0.5882 - val_loss: 0.7473 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6818 - acc: 0.5882\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6818 - acc: 0.5882 - val_loss: 0.7447 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5487 - acc: 0.8235\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5487 - acc: 0.8235 - val_loss: 0.7560 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6348 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6348 - acc: 0.6471 - val_loss: 0.7651 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5248 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5248 - acc: 0.6471 - val_loss: 0.7760 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4311 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4311 - acc: 0.8235 - val_loss: 0.7838 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7171 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7171 - acc: 0.6471 - val_loss: 0.7822 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4126 - acc: 0.7647\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4126 - acc: 0.7647 - val_loss: 0.7784 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5481 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5481 - acc: 0.7647 - val_loss: 0.7675 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4774 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4774 - acc: 0.8235 - val_loss: 0.7638 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5700 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5700 - acc: 0.7059 - val_loss: 0.7612 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5625 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5625 - acc: 0.7647 - val_loss: 0.7635 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5930 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5930 - acc: 0.5882 - val_loss: 0.7770 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6659 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6659 - acc: 0.6471 - val_loss: 0.7658 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6508 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6508 - acc: 0.5882 - val_loss: 0.7511 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6705 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6705 - acc: 0.5882 - val_loss: 0.7326 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7040 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7040 - acc: 0.5882 - val_loss: 0.7304 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6930 - acc: 0.5882\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6930 - acc: 0.5882 - val_loss: 0.7412 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6275 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6275 - acc: 0.6471 - val_loss: 0.7456 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7013 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.7013 - acc: 0.7059 - val_loss: 0.7486 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5821 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5821 - acc: 0.7647 - val_loss: 0.7476 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5469 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5469 - acc: 0.7647 - val_loss: 0.7458 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5473 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5473 - acc: 0.7059 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5628 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5628 - acc: 0.7059 - val_loss: 0.6672 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5823 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5823 - acc: 0.6471 - val_loss: 0.6456 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6654 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6654 - acc: 0.5882 - val_loss: 0.6623 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6160 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6160 - acc: 0.7059 - val_loss: 0.6822 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5019 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5019 - acc: 0.8235 - val_loss: 0.6903 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6091 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6091 - acc: 0.7059 - val_loss: 0.7064 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4738 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4738 - acc: 0.7647 - val_loss: 0.7042 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5685 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5685 - acc: 0.7059 - val_loss: 0.6979 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5754 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5754 - acc: 0.7059 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4653 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4653 - acc: 0.7059 - val_loss: 0.7712 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3651 - acc: 0.8824\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3651 - acc: 0.8824 - val_loss: 0.7795 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5312 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5312 - acc: 0.7059 - val_loss: 0.7886 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4284 - acc: 0.7647 - val_loss: 0.8001 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4530 - acc: 0.8235\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4530 - acc: 0.8235 - val_loss: 0.8121 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4622 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4622 - acc: 0.7059 - val_loss: 0.8271 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3926 - acc: 0.8824\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3926 - acc: 0.8824 - val_loss: 0.8455 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5068 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5068 - acc: 0.8235 - val_loss: 0.8616 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4845 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4845 - acc: 0.8235 - val_loss: 0.8721 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3646 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3646 - acc: 0.8235 - val_loss: 0.8830 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8698 - acc: 0.6471\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8698 - acc: 0.6471 - val_loss: 0.8523 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9262 - acc: 0.5294\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.9262 - acc: 0.5294 - val_loss: 0.8605 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6596 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6596 - acc: 0.6471 - val_loss: 0.8626 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5526 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5526 - acc: 0.5882 - val_loss: 0.8583 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6612 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6612 - acc: 0.6471 - val_loss: 0.8490 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8037 - acc: 0.4706\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.8037 - acc: 0.4706 - val_loss: 0.8318 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6598 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6598 - acc: 0.6471 - val_loss: 0.8173 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5294\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6949 - acc: 0.5294 - val_loss: 0.8037 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6665 - acc: 0.5882\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6665 - acc: 0.5882 - val_loss: 0.7939 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6501 - acc: 0.5294\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6501 - acc: 0.5294 - val_loss: 0.7854 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6145 - acc: 0.4706\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6145 - acc: 0.4706 - val_loss: 0.7695 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5559 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5559 - acc: 0.7059 - val_loss: 0.7521 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5631 - acc: 0.8235\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5631 - acc: 0.8235 - val_loss: 0.7263 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5047 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5047 - acc: 0.8235 - val_loss: 0.6985 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5331 - acc: 0.8824\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5331 - acc: 0.8824 - val_loss: 0.6833 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5444 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5444 - acc: 0.8235 - val_loss: 0.6851 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5788 - acc: 0.8235\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5788 - acc: 0.8235 - val_loss: 0.6977 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5142 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5142 - acc: 0.7647 - val_loss: 0.7194 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4861 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4861 - acc: 0.8235 - val_loss: 0.7418 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4786 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4786 - acc: 0.8235 - val_loss: 0.7585 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6241 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6241 - acc: 0.7059 - val_loss: 0.7713 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6437 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6437 - acc: 0.6471 - val_loss: 0.7849 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6084 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6084 - acc: 0.5882 - val_loss: 0.7942 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5979 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5979 - acc: 0.5882 - val_loss: 0.8009 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6380 - acc: 0.5294\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6380 - acc: 0.5294 - val_loss: 0.8059 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6539 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6539 - acc: 0.6471 - val_loss: 0.8088 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6405 - acc: 0.4706\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6405 - acc: 0.4706 - val_loss: 0.8098 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5848 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5848 - acc: 0.5882 - val_loss: 0.8090 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6062 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6062 - acc: 0.6471 - val_loss: 0.8050 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5942 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5942 - acc: 0.6471 - val_loss: 0.7986 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5612 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5612 - acc: 0.7059 - val_loss: 0.7875 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6274 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6274 - acc: 0.6471 - val_loss: 0.7887 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6072 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6072 - acc: 0.6471 - val_loss: 0.8000 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6075 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6075 - acc: 0.7059 - val_loss: 0.8145 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6028 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6028 - acc: 0.5882 - val_loss: 0.8264 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5683 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5683 - acc: 0.6471 - val_loss: 0.8360 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5589 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5589 - acc: 0.7059 - val_loss: 0.8437 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6012 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6012 - acc: 0.7059 - val_loss: 0.8492 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5961 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5961 - acc: 0.6471 - val_loss: 0.8530 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5055 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5055 - acc: 0.8235 - val_loss: 0.8559 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7978 - acc: 0.4706\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7978 - acc: 0.4706 - val_loss: 0.8421 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6594 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6594 - acc: 0.6471 - val_loss: 0.8503 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6401 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6401 - acc: 0.5882 - val_loss: 0.8563 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6333 - acc: 0.6471\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6333 - acc: 0.6471 - val_loss: 0.8596 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6727 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6727 - acc: 0.5882 - val_loss: 0.8604 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6734 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6734 - acc: 0.5294 - val_loss: 0.8611 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7752 - acc: 0.4706\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7752 - acc: 0.4706 - val_loss: 0.8636 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6834 - acc: 0.5882\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6834 - acc: 0.5882 - val_loss: 0.8602 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6836 - acc: 0.5882\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6836 - acc: 0.5882 - val_loss: 0.8565 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6870 - acc: 0.4706\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6870 - acc: 0.4706 - val_loss: 0.8511 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5326 - acc: 0.8235\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5326 - acc: 0.8235 - val_loss: 0.8453 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5500 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5500 - acc: 0.7647 - val_loss: 0.8141 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4839 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4839 - acc: 0.7647 - val_loss: 0.7389 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5061 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5061 - acc: 0.7059 - val_loss: 0.6971 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5407 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5407 - acc: 0.7059 - val_loss: 0.7216 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5799 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5799 - acc: 0.7059 - val_loss: 0.8321 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6145 - acc: 0.5294\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6145 - acc: 0.5294 - val_loss: 0.8614 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5652 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5652 - acc: 0.7647 - val_loss: 0.8696 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5668 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5668 - acc: 0.7059 - val_loss: 0.8737 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5828 - acc: 0.7059 - val_loss: 0.8750 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5556 - acc: 0.6471\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5556 - acc: 0.6471 - val_loss: 0.8626 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6663 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6663 - acc: 0.6471 - val_loss: 0.8615 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5808 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5808 - acc: 0.6471 - val_loss: 0.8872 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5096 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.5096 - acc: 0.7647 - val_loss: 0.9047 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6799 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6799 - acc: 0.7059 - val_loss: 0.9214 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6730 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6730 - acc: 0.6471 - val_loss: 0.9240 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5832 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5832 - acc: 0.6471 - val_loss: 0.9192 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5062 - acc: 0.8824\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.5062 - acc: 0.8824 - val_loss: 0.9067 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5308 - acc: 0.7647\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5308 - acc: 0.7647 - val_loss: 0.8875 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5419 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5419 - acc: 0.7059 - val_loss: 0.8945 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4953 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.4953 - acc: 0.7059 - val_loss: 0.9204 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5411 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5411 - acc: 0.7647 - val_loss: 0.9375 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5109 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5109 - acc: 0.7647 - val_loss: 0.9604 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4313 - acc: 0.8235\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4313 - acc: 0.8235 - val_loss: 0.9868 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4147 - acc: 0.7647\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4147 - acc: 0.7647 - val_loss: 1.0122 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4450 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4450 - acc: 0.8235 - val_loss: 1.0412 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5926 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.5926 - acc: 0.7059 - val_loss: 1.0533 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4461 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4461 - acc: 0.7647 - val_loss: 1.0582 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4223 - acc: 0.8824\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4223 - acc: 0.8824 - val_loss: 1.0740 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4546 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4546 - acc: 0.7059 - val_loss: 1.1003 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5585 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5585 - acc: 0.5882 - val_loss: 1.1596 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6361 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6361 - acc: 0.6471 - val_loss: 1.2401 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4829 - acc: 0.8235\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4829 - acc: 0.8235 - val_loss: 1.3294 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5441 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5441 - acc: 0.7059 - val_loss: 1.3499 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6208 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6208 - acc: 0.6471 - val_loss: 1.3242 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5314 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.5314 - acc: 0.7059 - val_loss: 1.2509 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5101 - acc: 0.6471\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5101 - acc: 0.6471 - val_loss: 1.2174 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4843 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4843 - acc: 0.7059 - val_loss: 1.2892 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5740 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5740 - acc: 0.6471 - val_loss: 1.4207 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5230 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5230 - acc: 0.7647 - val_loss: 1.5334 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5528 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5528 - acc: 0.7059 - val_loss: 1.5170 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5610 - acc: 0.7059\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5610 - acc: 0.7059 - val_loss: 1.4961 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5139 - acc: 0.8235\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5139 - acc: 0.8235 - val_loss: 1.4876 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8497 - acc: 0.5294\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.8497 - acc: 0.5294 - val_loss: 1.2300 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4425 - acc: 0.7059\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4425 - acc: 0.7059 - val_loss: 0.9621 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4995 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.4995 - acc: 0.6471 - val_loss: 0.7622 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5957 - acc: 0.5882\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5957 - acc: 0.5882 - val_loss: 0.6988 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6455 - acc: 0.6471\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6455 - acc: 0.6471 - val_loss: 0.7123 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5819 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5819 - acc: 0.6471 - val_loss: 0.7505 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5223 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5223 - acc: 0.6471 - val_loss: 0.8074 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5765 - acc: 0.5882\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5765 - acc: 0.5882 - val_loss: 0.9044 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6067 - acc: 0.5882\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6067 - acc: 0.5882 - val_loss: 0.9534 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6051 - acc: 0.6471\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6051 - acc: 0.6471 - val_loss: 0.9915 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6129 - acc: 0.4706\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6129 - acc: 0.4706 - val_loss: 1.0478 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6165 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6165 - acc: 0.5882 - val_loss: 1.0851 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5891 - acc: 0.5294\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5891 - acc: 0.5294 - val_loss: 1.0987 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5491 - acc: 0.7059\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5491 - acc: 0.7059 - val_loss: 1.1103 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5348 - acc: 0.7059\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5348 - acc: 0.7059 - val_loss: 1.1280 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5593 - acc: 0.6471\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5593 - acc: 0.6471 - val_loss: 1.0986 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5994 - acc: 0.5882\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5994 - acc: 0.5882 - val_loss: 0.9583 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6941 - acc: 0.5294\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6941 - acc: 0.5294 - val_loss: 0.8441 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6058 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6058 - acc: 0.6471 - val_loss: 0.8711 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5599 - acc: 0.7059\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5599 - acc: 0.7059 - val_loss: 0.9729 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5766 - acc: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5766 - acc: 0.5882 - val_loss: 1.1354 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5835 - acc: 0.5882\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5835 - acc: 0.5882 - val_loss: 1.1778 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6288 - acc: 0.6471\n",
      "Epoch 00006: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6288 - acc: 0.6471 - val_loss: 0.9616 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5318 - acc: 0.7647\n",
      "Epoch 00007: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5318 - acc: 0.7647 - val_loss: 0.8636 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6176 - acc: 0.7647\n",
      "Epoch 00008: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6176 - acc: 0.7647 - val_loss: 0.8079 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6486 - acc: 0.5294\n",
      "Epoch 00009: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6486 - acc: 0.5294 - val_loss: 0.8174 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6658 - acc: 0.7059\n",
      "Epoch 00010: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6658 - acc: 0.7059 - val_loss: 0.9010 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5850 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.5850 - acc: 0.7059 - val_loss: 0.8932 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6073 - acc: 0.6471\n",
      "Epoch 00002: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6073 - acc: 0.6471 - val_loss: 1.0280 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7308 - acc: 0.5882\n",
      "Epoch 00003: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7308 - acc: 0.5882 - val_loss: 0.8560 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6259 - acc: 0.6471\n",
      "Epoch 00004: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6259 - acc: 0.6471 - val_loss: 0.6905 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6143 - acc: 0.6471\n",
      "Epoch 00005: val_loss did not improve from 0.55683\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6143 - acc: 0.6471 - val_loss: 0.6016 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6369 - acc: 0.5882\n",
      "Epoch 00006: val_loss improved from 0.55683 to 0.53578, saving model to model04_06_0.54.h5\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.6369 - acc: 0.5882 - val_loss: 0.5358 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6242 - acc: 0.7059\n",
      "Epoch 00007: val_loss improved from 0.53578 to 0.49994, saving model to model04_07_0.50.h5\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.6242 - acc: 0.7059 - val_loss: 0.4999 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6194 - acc: 0.8235\n",
      "Epoch 00008: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6194 - acc: 0.8235 - val_loss: 0.5180 - val_acc: 0.8333\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6480 - acc: 0.7059\n",
      "Epoch 00009: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6480 - acc: 0.7059 - val_loss: 0.5388 - val_acc: 0.8333\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6137 - acc: 0.6471\n",
      "Epoch 00010: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6137 - acc: 0.6471 - val_loss: 0.5533 - val_acc: 0.8333\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4999 - acc: 0.7059\n",
      "Epoch 00001: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4999 - acc: 0.7059 - val_loss: 0.6771 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5029 - acc: 0.7647\n",
      "Epoch 00002: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5029 - acc: 0.7647 - val_loss: 0.6740 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4235 - acc: 0.8235\n",
      "Epoch 00003: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4235 - acc: 0.8235 - val_loss: 0.6691 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3750 - acc: 0.7647\n",
      "Epoch 00004: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3750 - acc: 0.7647 - val_loss: 0.6622 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3801 - acc: 0.8824\n",
      "Epoch 00005: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3801 - acc: 0.8824 - val_loss: 0.6531 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3767 - acc: 0.8235\n",
      "Epoch 00006: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.3767 - acc: 0.8235 - val_loss: 0.6347 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3842 - acc: 0.8824\n",
      "Epoch 00007: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3842 - acc: 0.8824 - val_loss: 0.5919 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3327 - acc: 0.9412\n",
      "Epoch 00008: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3327 - acc: 0.9412 - val_loss: 0.5553 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3610 - acc: 0.8235\n",
      "Epoch 00009: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3610 - acc: 0.8235 - val_loss: 0.5939 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5040 - acc: 0.7647\n",
      "Epoch 00010: val_loss did not improve from 0.49994\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5040 - acc: 0.7647 - val_loss: 0.7237 - val_acc: 0.5000\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4110 - acc: 0.8235\n",
      "Epoch 00001: val_loss improved from 0.49994 to 0.41888, saving model to model04_01_0.42.h5\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.4110 - acc: 0.8235 - val_loss: 0.4189 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3921 - acc: 0.8235\n",
      "Epoch 00002: val_loss did not improve from 0.41888\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3921 - acc: 0.8235 - val_loss: 0.6224 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6051 - acc: 0.7647\n",
      "Epoch 00003: val_loss did not improve from 0.41888\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6051 - acc: 0.7647 - val_loss: 0.7414 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4653 - acc: 0.7059\n",
      "Epoch 00004: val_loss did not improve from 0.41888\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4653 - acc: 0.7059 - val_loss: 0.8233 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3514 - acc: 0.8824\n",
      "Epoch 00005: val_loss did not improve from 0.41888\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3514 - acc: 0.8824 - val_loss: 0.8700 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5731 - acc: 0.7059\n",
      "Epoch 00006: val_loss did not improve from 0.41888\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5731 - acc: 0.7059 - val_loss: 0.4205 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3857 - acc: 0.8824\n",
      "Epoch 00007: val_loss improved from 0.41888 to 0.21917, saving model to model04_07_0.22.h5\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.3857 - acc: 0.8824 - val_loss: 0.2192 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3746 - acc: 0.8824\n",
      "Epoch 00008: val_loss improved from 0.21917 to 0.14331, saving model to model04_08_0.14.h5\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.3746 - acc: 0.8824 - val_loss: 0.1433 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3105 - acc: 0.8235\n",
      "Epoch 00009: val_loss improved from 0.14331 to 0.11584, saving model to model04_09_0.12.h5\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.3105 - acc: 0.8235 - val_loss: 0.1158 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5167 - acc: 0.8235\n",
      "Epoch 00010: val_loss did not improve from 0.11584\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5167 - acc: 0.8235 - val_loss: 0.1285 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_OF_TRAIN):\n",
    "    # 创建数据集\n",
    "    X_Train_Positive,X_Test_Positive,Y_Train_Positive,Y_Test_Positive \\\n",
    "            = train_test_split(Positive_img_data,Positive_lab_data,test_size=0.25)\n",
    "\n",
    "    # 把超多数据的Negative随机抽取到匹配的数量级\n",
    "    X_Train_Negative,X_Test_Negative,Y_Train_Negative,Y_Test_Negative \\\n",
    "        = train_test_split(Negative_img_data,Negative_lab_data,test_size=0.25)\n",
    "\n",
    "    X_Train_Negative,X_Test_Negative,Y_Train_Negative,Y_Test_Negative \\\n",
    "        = train_test_split(X_Test_Negative,Y_Test_Negative,test_size=0.25)\n",
    "    X_Train_Negative,X_Test_Negative,Y_Train_Negative,Y_Test_Negative \\\n",
    "        = train_test_split(X_Test_Negative,Y_Test_Negative,test_size=0.25)\n",
    "    X_Train_Negative,X_Test_Negative,Y_Train_Negative,Y_Test_Negative \\\n",
    "        = train_test_split(X_Test_Negative,Y_Test_Negative,test_size=0.25)\n",
    "\n",
    "    X_Train_Negative,X_Test_Negative,Y_Train_Negative,Y_Test_Negative \\\n",
    "        = train_test_split(X_Test_Negative,Y_Test_Negative,test_size=0.25)\n",
    "        \n",
    "    X_train=X_Train_Positive+X_Train_Negative\n",
    "    Y_train=Y_Train_Positive+Y_Train_Negative\n",
    "    X_test =X_Test_Positive+X_Test_Negative\n",
    "    Y_test =Y_Test_Positive+Y_Test_Negative\n",
    "\n",
    "    TrainData=[]\n",
    "    TestData=[]\n",
    "    for i in range(len(X_train)):\n",
    "        TrainData.append([X_train[i],Y_train[i]])\n",
    "    for i in range(len(X_test)):\n",
    "        TestData.append([X_test[i],Y_test[i]])\n",
    "        \n",
    "    # 对数据进行随机打乱顺序\n",
    "    random.shuffle(TrainData)\n",
    "    random.shuffle(TestData)\n",
    "\n",
    "    # 预处理后的数据\n",
    "    TrainData = TrainData_process(TrainData)\n",
    "    TestData = TestData_process(TestData)\n",
    "\n",
    "    TotalOfTrain=len(X_train)\n",
    "    TotalOfTest=len(X_test)\n",
    "\n",
    "    BATCH_SIZE_OF_TRAIN=TotalOfTrain\n",
    "    BATCH_SIZE_OF_TEST =TotalOfTest\n",
    "    # 利用python的生成器，这个生成数据的batch并经行训练\n",
    "    # 一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）\n",
    "    history = model.fit_generator(\n",
    "        # train_data_gen,                             # 生成器函数，输出为（inputs,targets)\n",
    "        # TrainData_process(training_data),             # 每次都进行数据增强\n",
    "        TrainData,\n",
    "        steps_per_epoch=TotalOfTrain // BATCH_SIZE_OF_TRAIN,    # 当生成器返回steps_per_epoch次数据时计一个epoch结束，执行下一个epoch\n",
    "        epochs= EPOCH,                                   # 数据迭代的轮数\n",
    "        validation_data=TestData,                 # 生成验证集的生成器\n",
    "        validation_steps=TotalOfTest // BATCH_SIZE_OF_TEST,     # 指定验证集生成器返回次数\n",
    "        callbacks = callbacks                         # 在训练时调用的一系列回调函数\n",
    "    )\n",
    "\n",
    "\n",
    "    ACC= ACC + history.history['acc']\n",
    "    VAL_ACC = VAL_ACC + history.history['val_acc']\n",
    "    LOSS = LOSS +  history.history['loss']\n",
    "    VAL_LOSS = VAL_LOSS + history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-02-07T18:44:42.953471</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.0, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m091d659d17\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.994149\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(110.450399 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.988865\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(171.445115 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.983582\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(232.439832 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.978299\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(293.434549 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.973015\" xlink:href=\"#m091d659d17\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(354.429265 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m7d0bb28f33\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"228.227219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(20.878125 232.026438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"200.223582\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 204.022801)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"172.219946\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(20.878125 176.019164)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"144.216309\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 148.015528)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"116.212672\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(20.878125 120.011891)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"88.209035\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 92.008254)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"60.205398\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.9 -->\r\n      <g transform=\"translate(20.878125 64.004617)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m7d0bb28f33\" y=\"32.201761\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p484cc4a160)\" d=\"M 58.999432 147.510847 \r\nL 59.609379 163.983574 \r\nL 60.219326 163.983574 \r\nL 60.829273 180.456309 \r\nL 61.43922 213.401762 \r\nL 62.049168 147.510847 \r\nL 62.659115 114.565394 \r\nL 63.269062 180.456309 \r\nL 63.879009 147.510847 \r\nL 64.488956 229.874489 \r\nL 65.098903 163.983574 \r\nL 65.708851 180.456309 \r\nL 66.318798 180.456309 \r\nL 66.928745 114.565394 \r\nL 68.148639 114.565394 \r\nL 68.758586 98.092668 \r\nL 69.368534 147.510847 \r\nL 69.978481 163.983574 \r\nL 70.588428 98.092668 \r\nL 71.198375 114.565394 \r\nL 71.808322 114.565394 \r\nL 72.418269 147.510847 \r\nL 73.638164 147.510847 \r\nL 74.858058 114.565394 \r\nL 75.468005 81.619941 \r\nL 76.077952 98.092668 \r\nL 76.6879 48.674488 \r\nL 77.907794 114.565394 \r\nL 78.517741 98.092668 \r\nL 79.127688 98.092668 \r\nL 79.737635 114.565394 \r\nL 80.347583 196.929035 \r\nL 80.95753 131.038121 \r\nL 81.567477 114.565394 \r\nL 82.177424 81.619941 \r\nL 82.787371 114.565394 \r\nL 83.397319 65.147215 \r\nL 84.007266 98.092668 \r\nL 84.617213 98.092668 \r\nL 85.22716 81.619941 \r\nL 85.837107 81.619941 \r\nL 86.447054 65.147215 \r\nL 87.057002 114.565394 \r\nL 87.666949 81.619941 \r\nL 88.276896 131.038121 \r\nL 88.886843 65.147215 \r\nL 89.49679 147.510847 \r\nL 90.106737 180.456309 \r\nL 91.326632 147.510847 \r\nL 91.936579 114.565394 \r\nL 92.546526 131.038121 \r\nL 93.156473 114.565394 \r\nL 93.76642 131.038121 \r\nL 94.376368 98.092668 \r\nL 94.986315 114.565394 \r\nL 95.596262 98.092668 \r\nL 96.206209 98.092668 \r\nL 96.816156 81.619941 \r\nL 97.426103 114.565394 \r\nL 98.645998 81.619941 \r\nL 99.255945 98.092668 \r\nL 99.865892 98.092668 \r\nL 101.085786 131.038121 \r\nL 101.695734 81.619941 \r\nL 102.305681 81.619941 \r\nL 103.525575 114.565394 \r\nL 104.135522 81.619941 \r\nL 104.745469 98.092668 \r\nL 105.355417 98.092668 \r\nL 105.965364 114.565394 \r\nL 106.575311 98.092668 \r\nL 107.185258 114.565394 \r\nL 107.795205 180.456309 \r\nL 108.405152 180.456309 \r\nL 109.0151 163.983574 \r\nL 113.28473 163.983574 \r\nL 113.894677 147.510847 \r\nL 114.504624 147.510847 \r\nL 115.114571 65.147215 \r\nL 115.724518 114.565394 \r\nL 116.334466 147.510847 \r\nL 116.944413 147.510847 \r\nL 117.55436 131.038121 \r\nL 118.164307 147.510847 \r\nL 118.774254 131.038121 \r\nL 119.384201 131.038121 \r\nL 119.994149 196.929035 \r\nL 120.604096 180.456309 \r\nL 121.214043 180.456309 \r\nL 121.82399 147.510847 \r\nL 123.043884 147.510847 \r\nL 123.653832 163.983574 \r\nL 132.803039 163.983574 \r\nL 133.412986 147.510847 \r\nL 134.022933 163.983574 \r\nL 134.632881 147.510847 \r\nL 135.242828 163.983574 \r\nL 135.852775 98.092668 \r\nL 136.462722 131.038121 \r\nL 137.072669 114.565394 \r\nL 137.682616 163.983574 \r\nL 138.292564 180.456309 \r\nL 138.902511 180.456309 \r\nL 139.512458 163.983574 \r\nL 140.122405 131.038121 \r\nL 140.732352 114.565394 \r\nL 141.342299 147.510847 \r\nL 141.952247 163.983574 \r\nL 142.562194 131.038121 \r\nL 143.172141 147.510847 \r\nL 143.782088 147.510847 \r\nL 144.392035 163.983574 \r\nL 145.61193 131.038121 \r\nL 146.221877 81.619941 \r\nL 146.831824 114.565394 \r\nL 147.441771 98.092668 \r\nL 148.051718 65.147215 \r\nL 148.661665 131.038121 \r\nL 149.88156 98.092668 \r\nL 150.491507 131.038121 \r\nL 151.101454 114.565394 \r\nL 151.711401 131.038121 \r\nL 152.321348 98.092668 \r\nL 152.931296 147.510847 \r\nL 153.541243 163.983574 \r\nL 156.590979 163.983574 \r\nL 157.200926 147.510847 \r\nL 158.42082 147.510847 \r\nL 159.640714 114.565394 \r\nL 160.250662 81.619941 \r\nL 160.860609 98.092668 \r\nL 162.080503 163.983574 \r\nL 162.69045 81.619941 \r\nL 163.300397 98.092668 \r\nL 163.910345 98.092668 \r\nL 164.520292 114.565394 \r\nL 165.130239 48.674488 \r\nL 165.740186 81.619941 \r\nL 166.350133 81.619941 \r\nL 166.96008 98.092668 \r\nL 167.570028 65.147215 \r\nL 168.179975 98.092668 \r\nL 168.789922 114.565394 \r\nL 170.619763 114.565394 \r\nL 171.839658 180.456309 \r\nL 172.449605 98.092668 \r\nL 173.059552 114.565394 \r\nL 173.669499 114.565394 \r\nL 174.279446 163.983574 \r\nL 174.889394 114.565394 \r\nL 175.499341 131.038121 \r\nL 176.109288 114.565394 \r\nL 176.719235 163.983574 \r\nL 177.329182 147.510847 \r\nL 177.939129 163.983574 \r\nL 178.549077 147.510847 \r\nL 179.159024 147.510847 \r\nL 179.768971 163.983574 \r\nL 180.378918 147.510847 \r\nL 181.598812 147.510847 \r\nL 182.20876 131.038121 \r\nL 182.818707 98.092668 \r\nL 183.428654 147.510847 \r\nL 184.038601 163.983574 \r\nL 184.648548 131.038121 \r\nL 185.258495 131.038121 \r\nL 185.868443 98.092668 \r\nL 186.47839 147.510847 \r\nL 188.918178 81.619941 \r\nL 189.528126 131.038121 \r\nL 190.138073 98.092668 \r\nL 190.74802 98.092668 \r\nL 191.357967 131.038121 \r\nL 191.967914 98.092668 \r\nL 192.577861 131.038121 \r\nL 193.187809 81.619941 \r\nL 193.797756 147.510847 \r\nL 194.407703 98.092668 \r\nL 195.01765 81.619941 \r\nL 196.237544 114.565394 \r\nL 196.847492 163.983574 \r\nL 197.457439 98.092668 \r\nL 198.067386 98.092668 \r\nL 198.677333 114.565394 \r\nL 199.28728 147.510847 \r\nL 199.897227 65.147215 \r\nL 200.507175 147.510847 \r\nL 201.117122 163.983574 \r\nL 201.727069 131.038121 \r\nL 202.337016 147.510847 \r\nL 202.946963 98.092668 \r\nL 203.55691 98.092668 \r\nL 204.166858 81.619941 \r\nL 204.776805 81.619941 \r\nL 205.386752 147.510847 \r\nL 205.996699 114.565394 \r\nL 206.606646 196.929035 \r\nL 207.216593 163.983574 \r\nL 207.826541 98.092668 \r\nL 208.436488 180.456309 \r\nL 209.046435 131.038121 \r\nL 209.656382 131.038121 \r\nL 210.266329 114.565394 \r\nL 211.486224 114.565394 \r\nL 212.096171 131.038121 \r\nL 212.706118 98.092668 \r\nL 213.316065 114.565394 \r\nL 213.926012 65.147215 \r\nL 214.535959 81.619941 \r\nL 215.145907 81.619941 \r\nL 215.755854 147.510847 \r\nL 216.365801 114.565394 \r\nL 216.975748 98.092668 \r\nL 217.585695 114.565394 \r\nL 218.195642 196.929035 \r\nL 218.80559 98.092668 \r\nL 219.415537 114.565394 \r\nL 220.025484 98.092668 \r\nL 220.635431 114.565394 \r\nL 221.245378 98.092668 \r\nL 221.855325 147.510847 \r\nL 222.465273 180.456309 \r\nL 223.07522 114.565394 \r\nL 223.685167 98.092668 \r\nL 224.295114 98.092668 \r\nL 224.905061 114.565394 \r\nL 225.515008 147.510847 \r\nL 226.734903 81.619941 \r\nL 227.34485 147.510847 \r\nL 227.954797 114.565394 \r\nL 228.564744 114.565394 \r\nL 229.174691 81.619941 \r\nL 229.784639 131.038121 \r\nL 231.004533 98.092668 \r\nL 231.61448 114.565394 \r\nL 232.224427 98.092668 \r\nL 233.444322 131.038121 \r\nL 234.054269 81.619941 \r\nL 234.664216 98.092668 \r\nL 235.88411 65.147215 \r\nL 236.494057 98.092668 \r\nL 237.104005 65.147215 \r\nL 237.713952 81.619941 \r\nL 238.323899 114.565394 \r\nL 239.543793 81.619941 \r\nL 240.763688 81.619941 \r\nL 241.373635 114.565394 \r\nL 241.983582 114.565394 \r\nL 242.593529 81.619941 \r\nL 243.203476 114.565394 \r\nL 243.813423 98.092668 \r\nL 244.423371 98.092668 \r\nL 245.033318 131.038121 \r\nL 245.643265 98.092668 \r\nL 246.253212 131.038121 \r\nL 246.863159 147.510847 \r\nL 247.473106 147.510847 \r\nL 248.083054 81.619941 \r\nL 248.693001 131.038121 \r\nL 249.302948 131.038121 \r\nL 249.912895 81.619941 \r\nL 250.522842 131.038121 \r\nL 251.132789 98.092668 \r\nL 251.742737 98.092668 \r\nL 252.352684 81.619941 \r\nL 252.962631 114.565394 \r\nL 253.572578 98.092668 \r\nL 254.182525 147.510847 \r\nL 254.792472 131.038121 \r\nL 255.40242 147.510847 \r\nL 257.232261 147.510847 \r\nL 259.062103 98.092668 \r\nL 259.67205 98.092668 \r\nL 260.281997 114.565394 \r\nL 260.891944 114.565394 \r\nL 262.111838 147.510847 \r\nL 263.331733 81.619941 \r\nL 263.94168 114.565394 \r\nL 264.551627 98.092668 \r\nL 265.161574 114.565394 \r\nL 266.381469 114.565394 \r\nL 266.991416 65.147215 \r\nL 267.601363 114.565394 \r\nL 268.821257 81.619941 \r\nL 269.431204 114.565394 \r\nL 270.041152 65.147215 \r\nL 270.651099 81.619941 \r\nL 271.870993 81.619941 \r\nL 272.48094 131.038121 \r\nL 273.090887 163.983574 \r\nL 273.700835 131.038121 \r\nL 274.310782 147.510847 \r\nL 274.920729 131.038121 \r\nL 275.530676 180.456309 \r\nL 276.140623 131.038121 \r\nL 276.75057 163.983574 \r\nL 277.360518 147.510847 \r\nL 278.580412 180.456309 \r\nL 279.190359 114.565394 \r\nL 279.800306 81.619941 \r\nL 280.410253 81.619941 \r\nL 281.020201 65.147215 \r\nL 281.630148 81.619941 \r\nL 282.240095 81.619941 \r\nL 282.850042 98.092668 \r\nL 283.459989 81.619941 \r\nL 284.069936 81.619941 \r\nL 284.679884 114.565394 \r\nL 285.899778 147.510847 \r\nL 286.509725 147.510847 \r\nL 287.119672 163.983574 \r\nL 287.729619 131.038121 \r\nL 288.339567 180.456309 \r\nL 288.949514 147.510847 \r\nL 289.559461 131.038121 \r\nL 290.169408 131.038121 \r\nL 290.779355 114.565394 \r\nL 291.389302 131.038121 \r\nL 291.99925 131.038121 \r\nL 292.609197 114.565394 \r\nL 293.219144 147.510847 \r\nL 294.439038 114.565394 \r\nL 295.048985 114.565394 \r\nL 295.658933 131.038121 \r\nL 296.26888 81.619941 \r\nL 296.878827 180.456309 \r\nL 297.488774 131.038121 \r\nL 298.098721 147.510847 \r\nL 298.708668 131.038121 \r\nL 300.53851 180.456309 \r\nL 301.148457 147.510847 \r\nL 301.758404 147.510847 \r\nL 302.368351 180.456309 \r\nL 302.978299 81.619941 \r\nL 303.588246 98.092668 \r\nL 304.198193 98.092668 \r\nL 304.80814 114.565394 \r\nL 306.028034 114.565394 \r\nL 306.637982 163.983574 \r\nL 307.247929 98.092668 \r\nL 307.857876 114.565394 \r\nL 308.467823 114.565394 \r\nL 309.07777 131.038121 \r\nL 310.297665 131.038121 \r\nL 310.907612 98.092668 \r\nL 312.127506 131.038121 \r\nL 312.737453 131.038121 \r\nL 313.3474 65.147215 \r\nL 313.957348 98.092668 \r\nL 314.567295 114.565394 \r\nL 315.177242 114.565394 \r\nL 315.787189 98.092668 \r\nL 316.397136 98.092668 \r\nL 317.007083 81.619941 \r\nL 317.617031 98.092668 \r\nL 318.226978 81.619941 \r\nL 318.836925 114.565394 \r\nL 319.446872 98.092668 \r\nL 320.056819 65.147215 \r\nL 320.666766 114.565394 \r\nL 321.276714 147.510847 \r\nL 321.886661 131.038121 \r\nL 322.496608 81.619941 \r\nL 323.106555 114.565394 \r\nL 323.716502 131.038121 \r\nL 324.326449 114.565394 \r\nL 324.936397 131.038121 \r\nL 325.546344 114.565394 \r\nL 326.156291 131.038121 \r\nL 326.766238 98.092668 \r\nL 327.376185 114.565394 \r\nL 327.986132 114.565394 \r\nL 328.59608 81.619941 \r\nL 329.206027 163.983574 \r\nL 329.815974 114.565394 \r\nL 331.035868 147.510847 \r\nL 331.645815 131.038121 \r\nL 332.86571 131.038121 \r\nL 333.475657 147.510847 \r\nL 334.085604 147.510847 \r\nL 334.695551 131.038121 \r\nL 335.305498 180.456309 \r\nL 335.915446 147.510847 \r\nL 336.525393 163.983574 \r\nL 337.13534 114.565394 \r\nL 337.745287 114.565394 \r\nL 339.575129 163.983574 \r\nL 340.185076 131.038121 \r\nL 340.795023 114.565394 \r\nL 341.40497 147.510847 \r\nL 342.014917 147.510847 \r\nL 342.624865 131.038121 \r\nL 343.234812 98.092668 \r\nL 343.844759 98.092668 \r\nL 344.454706 163.983574 \r\nL 345.064653 114.565394 \r\nL 345.6746 114.565394 \r\nL 346.894495 147.510847 \r\nL 347.504442 131.038121 \r\nL 348.114389 131.038121 \r\nL 348.724336 147.510847 \r\nL 349.944231 81.619941 \r\nL 350.554178 114.565394 \r\nL 351.164125 131.038121 \r\nL 352.993966 81.619941 \r\nL 353.603914 98.092668 \r\nL 354.213861 65.147215 \r\nL 354.823808 81.619941 \r\nL 356.043702 48.674488 \r\nL 356.653649 81.619941 \r\nL 357.263597 98.092668 \r\nL 357.873544 81.619941 \r\nL 358.483491 81.619941 \r\nL 359.703385 114.565394 \r\nL 360.313332 65.147215 \r\nL 360.92328 114.565394 \r\nL 361.533227 65.147215 \r\nL 362.143174 65.147215 \r\nL 362.753121 81.619941 \r\nL 363.363068 81.619941 \r\nL 363.363068 81.619941 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p484cc4a160)\" d=\"M 58.999432 172.219946 \r\nL 60.219326 172.219946 \r\nL 60.829273 125.547212 \r\nL 61.43922 172.219946 \r\nL 63.269062 172.219946 \r\nL 63.879009 125.547212 \r\nL 64.488956 125.547212 \r\nL 65.098903 172.219946 \r\nL 76.6879 172.219946 \r\nL 77.297847 125.547212 \r\nL 77.907794 172.219946 \r\nL 79.127688 172.219946 \r\nL 79.737635 125.547212 \r\nL 80.347583 172.219946 \r\nL 81.567477 172.219946 \r\nL 82.177424 125.547212 \r\nL 82.787371 125.547212 \r\nL 83.397319 172.219946 \r\nL 85.22716 172.219946 \r\nL 85.837107 218.892671 \r\nL 87.057002 218.892671 \r\nL 87.666949 172.219946 \r\nL 88.276896 172.219946 \r\nL 88.886843 218.892671 \r\nL 89.49679 172.219946 \r\nL 98.036051 172.219946 \r\nL 98.645998 125.547212 \r\nL 99.255945 172.219946 \r\nL 137.682616 172.219946 \r\nL 138.292564 125.547212 \r\nL 138.902511 125.547212 \r\nL 139.512458 172.219946 \r\nL 148.051718 172.219946 \r\nL 148.661665 125.547212 \r\nL 149.88156 125.547212 \r\nL 150.491507 172.219946 \r\nL 168.179975 172.219946 \r\nL 168.789922 125.547212 \r\nL 170.009816 125.547212 \r\nL 170.619763 172.219946 \r\nL 204.776805 172.219946 \r\nL 205.386752 125.547212 \r\nL 208.436488 125.547212 \r\nL 209.046435 172.219946 \r\nL 216.975748 172.219946 \r\nL 217.585695 125.547212 \r\nL 219.415537 125.547212 \r\nL 220.025484 172.219946 \r\nL 221.855325 172.219946 \r\nL 222.465273 125.547212 \r\nL 223.07522 125.547212 \r\nL 223.685167 172.219946 \r\nL 241.373635 172.219946 \r\nL 241.983582 125.547212 \r\nL 243.203476 125.547212 \r\nL 243.813423 172.219946 \r\nL 260.281997 172.219946 \r\nL 260.891944 125.547212 \r\nL 262.111838 125.547212 \r\nL 262.721786 172.219946 \r\nL 304.198193 172.219946 \r\nL 304.80814 125.547212 \r\nL 305.418087 125.547212 \r\nL 306.028034 172.219946 \r\nL 338.965181 172.219946 \r\nL 339.575129 125.547212 \r\nL 340.185076 172.219946 \r\nL 343.234812 172.219946 \r\nL 343.844759 125.547212 \r\nL 344.454706 172.219946 \r\nL 348.114389 172.219946 \r\nL 348.724336 125.547212 \r\nL 349.334283 125.547212 \r\nL 349.944231 78.874495 \r\nL 351.164125 78.874495 \r\nL 352.384019 172.219946 \r\nL 354.213861 172.219946 \r\nL 354.823808 125.547212 \r\nL 356.043702 125.547212 \r\nL 356.653649 172.219946 \r\nL 357.263597 172.219946 \r\nL 357.873544 125.547212 \r\nL 360.92328 125.547212 \r\nL 361.533227 32.201761 \r\nL 363.363068 32.201761 \r\nL 363.363068 32.201761 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- model accuracy -->\r\n    <g transform=\"translate(163.519688 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"404.443359\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"459.423828\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"514.404297\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"577.783203\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"618.896484\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"680.175781\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"735.15625\" xlink:href=\"#DejaVuSans-121\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 59.674375 \r\nL 132.015625 59.674375 \r\nQ 134.015625 59.674375 134.015625 57.674375 \r\nL 134.015625 29.318125 \r\nQ 134.015625 27.318125 132.015625 27.318125 \r\nL 50.78125 27.318125 \r\nQ 48.78125 27.318125 48.78125 29.318125 \r\nL 48.78125 57.674375 \r\nQ 48.78125 59.674375 50.78125 59.674375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 52.78125 35.416562 \r\nL 72.78125 35.416562 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_18\">\r\n     <!-- train -->\r\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 52.78125 50.094687 \r\nL 72.78125 50.094687 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- validation -->\r\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p484cc4a160\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-02-07T18:44:43.285619</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.0, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m06a0d9ab46\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.994149\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(110.450399 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.988865\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(171.445115 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.983582\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(232.439832 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.978299\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(293.434549 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.973015\" xlink:href=\"#m06a0d9ab46\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(354.429265 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m21166c4065\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"218.138851\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 221.93807)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"190.249506\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 194.048725)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"162.360162\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 166.15938)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"134.470817\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 138.270035)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"106.581472\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 110.380691)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"78.692127\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.2 -->\r\n      <g transform=\"translate(20.878125 82.491346)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"50.802782\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 1.4 -->\r\n      <g transform=\"translate(20.878125 54.602001)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m21166c4065\" y=\"22.913437\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 1.6 -->\r\n      <g transform=\"translate(20.878125 26.712656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pbdd3afba13)\" d=\"M 58.999432 149.597303 \r\nL 59.609379 151.31476 \r\nL 60.219326 150.276034 \r\nL 60.829273 139.660898 \r\nL 61.43922 145.025908 \r\nL 62.049168 161.662476 \r\nL 62.659115 152.550415 \r\nL 63.269062 150.658238 \r\nL 63.879009 149.420031 \r\nL 64.488956 144.32993 \r\nL 65.098903 150.920745 \r\nL 65.708851 153.469353 \r\nL 66.318798 149.314531 \r\nL 66.928745 159.98228 \r\nL 67.538692 157.180332 \r\nL 68.148639 159.013472 \r\nL 68.758586 165.848326 \r\nL 69.368534 156.280569 \r\nL 69.978481 144.844697 \r\nL 70.588428 179.305664 \r\nL 71.198375 178.912517 \r\nL 71.808322 151.809413 \r\nL 72.418269 155.592645 \r\nL 73.028217 148.768346 \r\nL 73.638164 166.930273 \r\nL 74.248111 149.487339 \r\nL 74.858058 173.996275 \r\nL 75.468005 187.853109 \r\nL 76.077952 179.863726 \r\nL 76.6879 187.633515 \r\nL 77.907794 164.345351 \r\nL 78.517741 172.182211 \r\nL 79.127688 156.542727 \r\nL 79.737635 175.265476 \r\nL 80.347583 143.062932 \r\nL 80.95753 169.670848 \r\nL 81.567477 169.180218 \r\nL 82.177424 175.114744 \r\nL 82.787371 175.657878 \r\nL 83.397319 165.737797 \r\nL 84.007266 165.446556 \r\nL 84.617213 176.487591 \r\nL 85.22716 174.651858 \r\nL 85.837107 179.213529 \r\nL 86.447054 190.654184 \r\nL 87.057002 173.803636 \r\nL 87.666949 179.507413 \r\nL 88.276896 182.27616 \r\nL 88.886843 178.545441 \r\nL 89.49679 134.419067 \r\nL 90.716685 137.928088 \r\nL 91.326632 158.24391 \r\nL 91.936579 155.34248 \r\nL 92.546526 156.980703 \r\nL 93.156473 162.500268 \r\nL 93.76642 160.8542 \r\nL 94.376368 162.264649 \r\nL 94.986315 170.772335 \r\nL 95.596262 165.053007 \r\nL 96.206209 164.514436 \r\nL 96.816156 165.127072 \r\nL 97.426103 161.67078 \r\nL 98.036051 171.252567 \r\nL 98.645998 176.210629 \r\nL 99.255945 163.967927 \r\nL 99.865892 171.223036 \r\nL 100.475839 176.672276 \r\nL 101.085786 162.048828 \r\nL 101.695734 178.846572 \r\nL 102.305681 178.611514 \r\nL 102.915628 182.044887 \r\nL 103.525575 164.209721 \r\nL 104.135522 182.114327 \r\nL 104.745469 174.925603 \r\nL 105.355417 175.83126 \r\nL 106.575311 185.974646 \r\nL 107.185258 181.418503 \r\nL 107.795205 109.766787 \r\nL 108.405152 138.727921 \r\nL 109.0151 126.137915 \r\nL 109.625047 118.625459 \r\nL 110.234994 133.337868 \r\nL 110.844941 136.431256 \r\nL 112.064835 146.415802 \r\nL 112.674783 148.608413 \r\nL 113.28473 149.859361 \r\nL 113.894677 153.943534 \r\nL 114.504624 150.081408 \r\nL 115.114571 155.716231 \r\nL 115.724518 155.893635 \r\nL 116.334466 154.417349 \r\nL 116.944413 153.848357 \r\nL 117.55436 154.560119 \r\nL 118.164307 154.018323 \r\nL 118.774254 158.239788 \r\nL 119.384201 161.490433 \r\nL 119.994149 144.881858 \r\nL 120.604096 153.137277 \r\nL 121.214043 149.036065 \r\nL 121.82399 152.089923 \r\nL 122.433937 152.387839 \r\nL 123.043884 151.356526 \r\nL 123.653832 149.245095 \r\nL 124.263779 149.688133 \r\nL 125.483673 149.766861 \r\nL 126.09362 149.221723 \r\nL 126.703567 149.053819 \r\nL 127.923462 149.755648 \r\nL 128.533409 149.2849 \r\nL 129.143356 149.581951 \r\nL 129.753303 149.519173 \r\nL 130.36325 149.237989 \r\nL 130.973198 149.105958 \r\nL 131.583145 149.483807 \r\nL 132.193092 149.999696 \r\nL 132.803039 150.296023 \r\nL 133.412986 151.298428 \r\nL 134.022933 151.462284 \r\nL 134.632881 152.231006 \r\nL 135.242828 152.79994 \r\nL 135.852775 160.15508 \r\nL 136.462722 159.940763 \r\nL 137.072669 156.204135 \r\nL 137.682616 163.444125 \r\nL 138.292564 155.371239 \r\nL 138.902511 141.201641 \r\nL 139.512458 151.947652 \r\nL 140.732352 154.205676 \r\nL 141.342299 150.534427 \r\nL 141.952247 148.542676 \r\nL 142.562194 154.163511 \r\nL 143.172141 149.560698 \r\nL 143.782088 148.13897 \r\nL 144.392035 148.870837 \r\nL 145.001982 150.555281 \r\nL 145.61193 150.719844 \r\nL 146.221877 158.302125 \r\nL 146.831824 154.229522 \r\nL 147.441771 155.43326 \r\nL 148.051718 159.987251 \r\nL 148.661665 161.692781 \r\nL 149.271613 143.63913 \r\nL 149.88156 147.766391 \r\nL 150.491507 146.286398 \r\nL 151.101454 152.204533 \r\nL 151.711401 153.201019 \r\nL 152.321348 156.317755 \r\nL 152.931296 150.720791 \r\nL 153.541243 152.137341 \r\nL 154.15119 151.068244 \r\nL 154.761137 151.67163 \r\nL 155.371084 149.391306 \r\nL 155.981031 151.134014 \r\nL 156.590979 154.206482 \r\nL 157.200926 155.534014 \r\nL 157.810873 153.793243 \r\nL 158.42082 153.244157 \r\nL 159.030767 149.21667 \r\nL 159.640714 158.424083 \r\nL 160.250662 159.264709 \r\nL 160.860609 157.221217 \r\nL 161.470556 153.752366 \r\nL 162.080503 149.146211 \r\nL 162.69045 162.521704 \r\nL 163.300397 172.73089 \r\nL 164.520292 163.25176 \r\nL 165.130239 181.311312 \r\nL 165.740186 175.194943 \r\nL 166.350133 178.444317 \r\nL 166.96008 151.516867 \r\nL 167.570028 166.639888 \r\nL 168.179975 175.556368 \r\nL 168.789922 143.539781 \r\nL 169.399869 141.768239 \r\nL 170.009816 157.982483 \r\nL 170.619763 156.212588 \r\nL 171.229711 143.498929 \r\nL 171.839658 143.377163 \r\nL 172.449605 163.006366 \r\nL 173.669499 160.103797 \r\nL 174.279446 142.074724 \r\nL 174.889394 146.376288 \r\nL 175.499341 153.892509 \r\nL 176.109288 157.276083 \r\nL 176.719235 151.753309 \r\nL 177.329182 151.988729 \r\nL 177.939129 153.921317 \r\nL 178.549077 153.721987 \r\nL 179.159024 152.762894 \r\nL 179.768971 152.747352 \r\nL 180.378918 153.473609 \r\nL 180.988865 157.547002 \r\nL 181.598812 151.21256 \r\nL 182.818707 160.680752 \r\nL 183.428654 152.413098 \r\nL 184.038601 158.27808 \r\nL 184.648548 154.905353 \r\nL 185.258495 159.591258 \r\nL 185.868443 161.671968 \r\nL 186.47839 159.673959 \r\nL 187.088337 156.241255 \r\nL 187.698284 161.775016 \r\nL 188.308231 162.622084 \r\nL 188.918178 164.185219 \r\nL 189.528126 158.000786 \r\nL 190.138073 168.099002 \r\nL 190.74802 168.822666 \r\nL 191.357967 157.362732 \r\nL 191.967914 165.964282 \r\nL 192.577861 158.879762 \r\nL 193.187809 173.505629 \r\nL 193.797756 160.985823 \r\nL 194.407703 178.512892 \r\nL 195.01765 178.336722 \r\nL 195.627597 175.544557 \r\nL 196.237544 165.190765 \r\nL 196.847492 150.403693 \r\nL 197.457439 171.980171 \r\nL 198.067386 172.831203 \r\nL 198.677333 162.938933 \r\nL 199.28728 133.322782 \r\nL 199.897227 179.339451 \r\nL 200.507175 140.969529 \r\nL 201.117122 141.782128 \r\nL 201.727069 158.736593 \r\nL 202.337016 153.771084 \r\nL 202.946963 176.61183 \r\nL 203.55691 166.781702 \r\nL 204.166858 181.897692 \r\nL 204.776805 170.138038 \r\nL 205.386752 149.209505 \r\nL 205.996699 154.712987 \r\nL 206.606646 127.629694 \r\nL 207.216593 150.569851 \r\nL 207.826541 159.713456 \r\nL 208.436488 149.781589 \r\nL 209.046435 160.531033 \r\nL 209.656382 160.929404 \r\nL 210.266329 157.299904 \r\nL 210.876276 150.063114 \r\nL 211.486224 168.960881 \r\nL 212.096171 160.021528 \r\nL 212.706118 163.300724 \r\nL 213.316065 164.593122 \r\nL 213.926012 170.718318 \r\nL 214.535959 168.916763 \r\nL 215.145907 170.712433 \r\nL 215.755854 167.547581 \r\nL 216.365801 156.702785 \r\nL 216.975748 172.280572 \r\nL 217.585695 172.217727 \r\nL 218.195642 143.776231 \r\nL 218.80559 160.406308 \r\nL 219.415537 149.904627 \r\nL 220.025484 177.174401 \r\nL 220.635431 158.474077 \r\nL 221.245378 180.406811 \r\nL 221.855325 147.075907 \r\nL 222.465273 126.898025 \r\nL 223.07522 147.790952 \r\nL 223.685167 174.210317 \r\nL 224.295114 161.980897 \r\nL 224.905061 167.397747 \r\nL 225.515008 164.297077 \r\nL 226.124956 167.485975 \r\nL 226.734903 167.370559 \r\nL 227.34485 164.663738 \r\nL 227.954797 158.860728 \r\nL 228.564744 172.314799 \r\nL 229.174691 181.390767 \r\nL 229.784639 150.100616 \r\nL 230.394586 162.928303 \r\nL 231.004533 167.514575 \r\nL 231.61448 165.311184 \r\nL 232.224427 171.085178 \r\nL 232.834374 163.498674 \r\nL 233.444322 158.443798 \r\nL 234.054269 172.144352 \r\nL 234.664216 167.713798 \r\nL 235.274163 178.125581 \r\nL 235.88411 194.117741 \r\nL 236.494057 186.910877 \r\nL 237.104005 194.149949 \r\nL 237.713952 185.846983 \r\nL 238.323899 183.570279 \r\nL 238.933846 182.178211 \r\nL 239.543793 179.722161 \r\nL 240.15374 191.817041 \r\nL 240.763688 191.46788 \r\nL 241.983582 157.079586 \r\nL 242.593529 188.772139 \r\nL 243.203476 142.754386 \r\nL 243.813423 169.467611 \r\nL 244.423371 172.819858 \r\nL 245.033318 172.93293 \r\nL 245.643265 170.788793 \r\nL 246.253212 155.120268 \r\nL 246.863159 159.280999 \r\nL 247.473106 150.949512 \r\nL 248.083054 169.51635 \r\nL 248.693001 157.501429 \r\nL 249.912895 185.907787 \r\nL 250.522842 146.024489 \r\nL 251.132789 188.490232 \r\nL 251.742737 169.593059 \r\nL 252.352684 179.451816 \r\nL 252.962631 166.54324 \r\nL 253.572578 167.582847 \r\nL 254.182525 163.332175 \r\nL 254.792472 153.174746 \r\nL 255.40242 155.278297 \r\nL 256.012367 152.535894 \r\nL 256.622314 147.8587 \r\nL 257.232261 149.394822 \r\nL 257.842208 158.530796 \r\nL 258.452155 148.236806 \r\nL 259.062103 164.856877 \r\nL 259.67205 169.770122 \r\nL 260.281997 169.707211 \r\nL 260.891944 167.547124 \r\nL 261.501891 164.824511 \r\nL 262.111838 153.235022 \r\nL 262.721786 160.135448 \r\nL 263.331733 176.038869 \r\nL 263.94168 161.096659 \r\nL 264.551627 179.95884 \r\nL 265.161574 166.747948 \r\nL 265.771521 165.785673 \r\nL 266.991416 195.120428 \r\nL 267.601363 171.95886 \r\nL 268.21131 186.287389 \r\nL 268.821257 182.853256 \r\nL 269.431204 181.580331 \r\nL 270.041152 191.281092 \r\nL 270.651099 175.355899 \r\nL 271.261046 178.468246 \r\nL 271.870993 195.18417 \r\nL 272.48094 124.738113 \r\nL 273.090887 116.877315 \r\nL 273.700835 154.048669 \r\nL 274.310782 168.969559 \r\nL 274.920729 153.831027 \r\nL 275.530676 133.948343 \r\nL 276.140623 154.022678 \r\nL 276.75057 149.131492 \r\nL 277.360518 153.084789 \r\nL 277.970465 155.367166 \r\nL 278.580412 160.342367 \r\nL 279.190359 168.513713 \r\nL 279.800306 167.511683 \r\nL 280.410253 175.655792 \r\nL 281.020201 171.694499 \r\nL 281.630148 170.113818 \r\nL 282.240095 165.322197 \r\nL 282.850042 174.330313 \r\nL 283.459989 178.244492 \r\nL 284.069936 179.286804 \r\nL 284.679884 158.993116 \r\nL 285.289831 156.271758 \r\nL 285.899778 161.190964 \r\nL 286.509725 162.657749 \r\nL 287.119672 157.057502 \r\nL 287.729619 154.847221 \r\nL 288.339567 156.706883 \r\nL 288.949514 164.474523 \r\nL 289.559461 161.492378 \r\nL 290.169408 163.168277 \r\nL 290.779355 167.764258 \r\nL 291.389302 158.54589 \r\nL 291.99925 161.359391 \r\nL 292.609197 161.30987 \r\nL 293.219144 161.970723 \r\nL 293.829091 166.780788 \r\nL 294.439038 168.084798 \r\nL 295.048985 162.188547 \r\nL 295.658933 162.900774 \r\nL 296.26888 175.533228 \r\nL 296.878827 134.782087 \r\nL 297.488774 154.083386 \r\nL 298.098721 156.768763 \r\nL 298.708668 157.71075 \r\nL 299.318616 152.221464 \r\nL 299.928563 152.119122 \r\nL 300.53851 137.932319 \r\nL 301.148457 150.726335 \r\nL 301.758404 150.701176 \r\nL 302.368351 150.221542 \r\nL 302.978299 171.756745 \r\nL 303.588246 169.338015 \r\nL 304.198193 178.545075 \r\nL 304.80814 175.454126 \r\nL 306.637982 160.332792 \r\nL 307.247929 167.217533 \r\nL 307.857876 166.996168 \r\nL 308.467823 164.764949 \r\nL 309.07777 168.552969 \r\nL 309.687717 153.121169 \r\nL 310.907612 174.968708 \r\nL 311.517559 151.218935 \r\nL 312.127506 152.17437 \r\nL 313.3474 175.436788 \r\nL 313.957348 172.014773 \r\nL 314.567295 170.467447 \r\nL 315.177242 176.960085 \r\nL 315.787189 170.580086 \r\nL 316.397136 174.790821 \r\nL 317.007083 185.878447 \r\nL 317.617031 188.203554 \r\nL 318.226978 183.978116 \r\nL 318.836925 163.390573 \r\nL 319.446872 183.822285 \r\nL 320.056819 187.144934 \r\nL 320.666766 182.638565 \r\nL 321.276714 168.142605 \r\nL 321.886661 157.32316 \r\nL 322.496608 178.694348 \r\nL 323.106555 170.161993 \r\nL 323.716502 159.457772 \r\nL 324.326449 171.930916 \r\nL 324.936397 174.894044 \r\nL 325.546344 178.494029 \r\nL 326.156291 165.978878 \r\nL 326.766238 173.101299 \r\nL 327.376185 168.944881 \r\nL 327.986132 167.793083 \r\nL 328.59608 174.36528 \r\nL 329.206027 127.533752 \r\nL 329.815974 184.322971 \r\nL 330.425921 176.367658 \r\nL 331.035868 162.956421 \r\nL 331.645815 156.013648 \r\nL 332.86571 173.192636 \r\nL 333.475657 165.643435 \r\nL 334.085604 161.425785 \r\nL 334.695551 161.6503 \r\nL 335.305498 160.566624 \r\nL 335.915446 160.060435 \r\nL 336.525393 163.878136 \r\nL 337.13534 169.45314 \r\nL 337.745287 171.44667 \r\nL 338.355234 168.041826 \r\nL 338.965181 162.44645 \r\nL 339.575129 149.23936 \r\nL 340.185076 161.557999 \r\nL 340.795023 167.951512 \r\nL 341.40497 165.619273 \r\nL 342.014917 164.662816 \r\nL 342.624865 158.348205 \r\nL 343.234812 171.871147 \r\nL 343.844759 159.908597 \r\nL 344.454706 155.584051 \r\nL 345.064653 153.189732 \r\nL 345.6746 164.451791 \r\nL 346.284548 161.347605 \r\nL 346.894495 144.117533 \r\nL 347.504442 158.755328 \r\nL 348.114389 160.370385 \r\nL 348.724336 157.214044 \r\nL 349.334283 158.985569 \r\nL 349.944231 159.658565 \r\nL 350.554178 155.667516 \r\nL 351.164125 160.446362 \r\nL 351.774072 176.322958 \r\nL 352.384019 175.905674 \r\nL 352.993966 186.970896 \r\nL 353.603914 193.739864 \r\nL 354.213861 193.023165 \r\nL 354.823808 193.496743 \r\nL 355.433755 192.455951 \r\nL 356.043702 199.630013 \r\nL 356.653649 195.685684 \r\nL 357.263597 175.745018 \r\nL 357.873544 188.722086 \r\nL 358.483491 191.355 \r\nL 359.093438 161.650948 \r\nL 359.703385 181.139883 \r\nL 360.313332 197.032383 \r\nL 360.92328 166.11661 \r\nL 361.533227 192.238219 \r\nL 362.143174 193.792551 \r\nL 362.753121 202.727944 \r\nL 363.363068 173.969844 \r\nL 363.363068 173.969844 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pbdd3afba13)\" d=\"M 58.999432 149.924841 \r\nL 59.609379 148.937921 \r\nL 60.219326 148.587426 \r\nL 60.829273 149.915332 \r\nL 61.43922 149.972284 \r\nL 62.049168 149.344894 \r\nL 62.659115 149.293154 \r\nL 63.879009 149.829098 \r\nL 64.488956 149.885194 \r\nL 65.098903 148.96584 \r\nL 65.708851 148.913285 \r\nL 67.538692 147.911305 \r\nL 68.148639 148.079217 \r\nL 68.758586 148.089698 \r\nL 69.368534 147.604272 \r\nL 69.978481 146.320733 \r\nL 70.588428 144.282212 \r\nL 71.198375 146.950559 \r\nL 72.418269 148.049977 \r\nL 73.028217 146.666565 \r\nL 73.638164 142.996538 \r\nL 74.248111 146.048019 \r\nL 74.858058 147.448602 \r\nL 76.077952 147.43877 \r\nL 76.6879 147.227154 \r\nL 77.297847 160.291674 \r\nL 77.907794 157.451135 \r\nL 78.517741 157.3303 \r\nL 79.127688 158.888082 \r\nL 79.737635 163.246124 \r\nL 80.347583 157.240035 \r\nL 80.95753 156.512373 \r\nL 81.567477 159.500502 \r\nL 82.177424 163.141098 \r\nL 82.787371 164.188385 \r\nL 83.397319 141.488593 \r\nL 84.007266 137.352314 \r\nL 84.617213 136.472806 \r\nL 85.22716 138.490589 \r\nL 85.837107 138.539021 \r\nL 86.447054 140.506128 \r\nL 87.057002 135.035651 \r\nL 87.666949 124.943876 \r\nL 88.276896 125.48288 \r\nL 88.886843 130.314189 \r\nL 89.49679 151.035297 \r\nL 90.106737 150.32164 \r\nL 90.716685 150.173858 \r\nL 92.546526 148.363294 \r\nL 93.156473 148.187876 \r\nL 94.986315 148.053725 \r\nL 95.596262 148.187934 \r\nL 96.816156 148.138637 \r\nL 97.426103 148.769385 \r\nL 98.036051 150.54401 \r\nL 98.645998 152.896845 \r\nL 99.865892 146.259069 \r\nL 100.475839 145.25329 \r\nL 101.085786 144.59395 \r\nL 101.695734 141.45626 \r\nL 102.915628 139.319554 \r\nL 104.745469 131.453628 \r\nL 105.355417 130.341859 \r\nL 105.965364 129.934986 \r\nL 106.575311 130.548479 \r\nL 107.185258 127.86631 \r\nL 107.795205 127.183249 \r\nL 108.405152 122.894791 \r\nL 109.0151 126.40634 \r\nL 110.234994 136.179005 \r\nL 110.844941 139.875979 \r\nL 111.454888 142.546511 \r\nL 112.064835 144.283484 \r\nL 112.674783 145.345749 \r\nL 113.28473 145.969532 \r\nL 113.894677 147.802339 \r\nL 114.504624 148.273444 \r\nL 115.724518 148.840749 \r\nL 117.55436 149.305023 \r\nL 118.164307 149.519464 \r\nL 118.774254 149.581677 \r\nL 119.384201 149.463343 \r\nL 119.994149 148.752413 \r\nL 120.604096 148.467239 \r\nL 123.043884 148.215944 \r\nL 127.313515 148.084944 \r\nL 131.583145 148.183279 \r\nL 135.242828 148.48348 \r\nL 135.852775 148.844863 \r\nL 136.462722 149.717631 \r\nL 137.682616 151.705982 \r\nL 138.292564 157.023848 \r\nL 138.902511 154.588703 \r\nL 139.512458 151.359269 \r\nL 140.122405 149.54464 \r\nL 140.732352 148.576729 \r\nL 141.342299 148.01299 \r\nL 141.952247 147.754214 \r\nL 142.562194 147.734865 \r\nL 143.782088 148.0154 \r\nL 146.221877 149.310625 \r\nL 146.831824 150.042675 \r\nL 147.441771 150.999008 \r\nL 148.661665 154.157685 \r\nL 149.271613 154.281296 \r\nL 149.88156 153.487589 \r\nL 150.491507 150.571605 \r\nL 151.101454 149.121584 \r\nL 151.711401 148.065262 \r\nL 152.321348 147.705732 \r\nL 153.541243 147.451254 \r\nL 155.981031 147.164543 \r\nL 156.590979 146.68589 \r\nL 157.200926 146.608791 \r\nL 158.42082 146.787126 \r\nL 159.030767 147.066456 \r\nL 159.640714 147.565423 \r\nL 160.250662 148.29941 \r\nL 161.470556 150.296572 \r\nL 162.080503 150.568937 \r\nL 162.69045 149.742034 \r\nL 163.300397 149.626867 \r\nL 165.130239 148.075269 \r\nL 165.740186 148.058347 \r\nL 166.350133 147.92137 \r\nL 166.96008 147.002939 \r\nL 168.179975 144.843691 \r\nL 168.789922 163.483996 \r\nL 169.399869 161.720517 \r\nL 170.619763 154.260891 \r\nL 171.229711 152.82017 \r\nL 171.839658 152.340695 \r\nL 172.449605 152.726523 \r\nL 173.059552 153.992814 \r\nL 173.669499 154.527994 \r\nL 174.279446 153.572277 \r\nL 174.889394 149.237681 \r\nL 175.499341 148.379036 \r\nL 176.109288 147.87332 \r\nL 176.719235 147.533373 \r\nL 177.329182 147.381668 \r\nL 178.549077 147.440166 \r\nL 179.768971 147.730742 \r\nL 180.378918 148.011593 \r\nL 180.988865 147.64209 \r\nL 181.598812 147.74158 \r\nL 184.038601 148.533384 \r\nL 185.868443 147.845667 \r\nL 186.47839 147.677348 \r\nL 187.088337 145.339889 \r\nL 188.308231 144.114724 \r\nL 188.918178 143.565098 \r\nL 189.528126 142.841234 \r\nL 190.138073 141.62271 \r\nL 190.74802 140.113909 \r\nL 191.357967 138.201941 \r\nL 192.577861 133.153457 \r\nL 193.187809 140.496354 \r\nL 193.797756 139.408314 \r\nL 194.407703 142.650049 \r\nL 195.01765 144.789008 \r\nL 195.627597 133.588091 \r\nL 196.237544 129.559057 \r\nL 196.847492 133.34967 \r\nL 197.457439 142.477566 \r\nL 198.067386 144.638567 \r\nL 198.677333 134.760834 \r\nL 199.28728 103.139226 \r\nL 199.897227 101.804887 \r\nL 200.507175 108.559392 \r\nL 201.117122 118.592736 \r\nL 201.727069 126.523468 \r\nL 202.337016 132.263343 \r\nL 204.166858 141.246224 \r\nL 204.776805 141.820727 \r\nL 205.386752 168.380128 \r\nL 205.996699 166.926707 \r\nL 207.216593 160.724612 \r\nL 207.826541 158.402481 \r\nL 208.436488 155.395351 \r\nL 209.046435 153.179459 \r\nL 209.656382 151.838429 \r\nL 210.266329 150.830023 \r\nL 210.876276 150.152689 \r\nL 211.486224 145.872452 \r\nL 212.706118 145.983986 \r\nL 213.926012 146.30293 \r\nL 214.535959 146.644731 \r\nL 215.145907 146.799103 \r\nL 215.755854 146.721107 \r\nL 216.975748 145.867423 \r\nL 217.585695 154.418014 \r\nL 218.80559 153.737771 \r\nL 219.415537 153.787981 \r\nL 220.635431 155.908405 \r\nL 221.245378 157.089669 \r\nL 221.855325 156.942677 \r\nL 222.465273 155.152043 \r\nL 223.07522 151.521422 \r\nL 223.685167 147.113052 \r\nL 224.295114 146.347372 \r\nL 224.905061 145.999554 \r\nL 225.515008 146.282383 \r\nL 226.124956 147.007261 \r\nL 226.734903 148.100695 \r\nL 227.34485 148.788843 \r\nL 228.564744 148.803131 \r\nL 229.174691 149.04328 \r\nL 229.784639 145.758282 \r\nL 230.394586 144.534696 \r\nL 231.004533 143.8523 \r\nL 231.61448 143.410916 \r\nL 232.834374 143.057571 \r\nL 233.444322 143.028895 \r\nL 234.054269 143.425495 \r\nL 234.664216 144.39063 \r\nL 235.274163 145.61405 \r\nL 235.88411 145.698122 \r\nL 236.494057 145.933243 \r\nL 237.104005 146.504471 \r\nL 237.713952 145.500945 \r\nL 238.933846 138.347878 \r\nL 239.543793 135.845275 \r\nL 240.15374 134.519729 \r\nL 240.763688 133.966147 \r\nL 241.373635 136.441313 \r\nL 241.983582 159.236756 \r\nL 242.593529 158.865175 \r\nL 243.813423 148.43366 \r\nL 244.423371 145.478462 \r\nL 245.033318 143.238873 \r\nL 245.643265 142.244141 \r\nL 246.253212 141.732432 \r\nL 246.863159 141.817286 \r\nL 247.473106 142.185552 \r\nL 248.083054 140.60699 \r\nL 248.693001 139.343267 \r\nL 249.302948 137.817344 \r\nL 249.912895 136.723328 \r\nL 250.522842 136.950519 \r\nL 251.132789 137.478394 \r\nL 251.742737 139.003436 \r\nL 252.962631 139.882279 \r\nL 253.572578 139.566203 \r\nL 254.182525 137.677201 \r\nL 254.792472 139.245314 \r\nL 255.40242 141.28381 \r\nL 256.012367 143.871915 \r\nL 256.622314 144.173305 \r\nL 257.232261 142.665027 \r\nL 257.842208 142.060054 \r\nL 258.452155 141.640613 \r\nL 259.67205 142.028037 \r\nL 260.281997 148.827966 \r\nL 260.891944 152.990285 \r\nL 261.501891 155.998238 \r\nL 262.111838 153.670637 \r\nL 262.721786 150.892494 \r\nL 263.331733 149.77105 \r\nL 263.94168 147.527156 \r\nL 264.551627 147.825354 \r\nL 265.771521 149.445839 \r\nL 266.381469 138.48911 \r\nL 267.601363 136.061503 \r\nL 268.821257 132.786687 \r\nL 269.431204 130.697681 \r\nL 270.651099 125.876064 \r\nL 271.870993 122.891974 \r\nL 272.48094 127.172502 \r\nL 273.090887 126.03209 \r\nL 273.700835 125.738672 \r\nL 274.310782 126.337777 \r\nL 274.920729 127.637257 \r\nL 276.140623 132.062774 \r\nL 276.75057 133.949682 \r\nL 277.970465 136.499811 \r\nL 279.190359 141.153799 \r\nL 280.410253 148.62182 \r\nL 281.020201 150.738296 \r\nL 281.630148 150.495761 \r\nL 282.240095 148.73677 \r\nL 283.459989 142.588626 \r\nL 284.069936 140.251791 \r\nL 285.289831 136.57986 \r\nL 285.899778 135.285226 \r\nL 286.509725 134.348974 \r\nL 287.119672 133.64766 \r\nL 287.729619 133.238277 \r\nL 288.339567 133.108017 \r\nL 288.949514 133.215004 \r\nL 289.559461 133.769243 \r\nL 290.169408 134.67096 \r\nL 290.779355 136.209193 \r\nL 291.389302 136.042602 \r\nL 291.99925 134.472095 \r\nL 293.219144 130.791911 \r\nL 294.439038 128.377636 \r\nL 295.048985 127.607601 \r\nL 296.26888 126.681914 \r\nL 296.878827 128.604736 \r\nL 297.488774 127.45952 \r\nL 298.098721 126.61946 \r\nL 298.708668 126.15768 \r\nL 299.928563 125.950528 \r\nL 300.53851 125.607322 \r\nL 301.758404 126.590003 \r\nL 302.978299 128.155839 \r\nL 303.588246 132.504091 \r\nL 304.198193 142.993945 \r\nL 304.80814 148.813271 \r\nL 305.418087 145.405352 \r\nL 306.028034 129.993126 \r\nL 306.637982 125.908014 \r\nL 307.247929 124.771667 \r\nL 307.857876 124.189384 \r\nL 308.467823 124.011647 \r\nL 309.07777 125.747191 \r\nL 309.687717 125.894715 \r\nL 310.297665 122.30487 \r\nL 311.517559 117.543737 \r\nL 312.127506 117.173477 \r\nL 312.737453 117.8424 \r\nL 313.3474 119.594293 \r\nL 313.957348 122.271408 \r\nL 314.567295 121.286292 \r\nL 315.177242 117.67925 \r\nL 315.787189 115.296593 \r\nL 317.007083 108.427893 \r\nL 318.226978 100.83021 \r\nL 318.836925 99.151252 \r\nL 319.446872 98.466952 \r\nL 320.056819 96.269064 \r\nL 320.666766 92.595105 \r\nL 321.276714 84.327351 \r\nL 322.496608 60.64349 \r\nL 323.106555 57.787433 \r\nL 323.716502 61.378258 \r\nL 324.326449 71.591725 \r\nL 324.936397 76.267855 \r\nL 325.546344 66.25849 \r\nL 326.766238 32.201761 \r\nL 327.376185 34.489333 \r\nL 327.986132 37.407994 \r\nL 328.59608 38.585009 \r\nL 329.815974 111.861553 \r\nL 330.425921 139.735777 \r\nL 331.035868 148.585406 \r\nL 331.645815 146.703278 \r\nL 332.255763 141.368015 \r\nL 332.86571 133.432937 \r\nL 333.475657 119.905814 \r\nL 334.085604 113.076537 \r\nL 334.695551 107.76931 \r\nL 335.305498 99.916092 \r\nL 335.915446 94.709777 \r\nL 337.745287 88.729014 \r\nL 338.355234 92.829162 \r\nL 338.965181 112.391522 \r\nL 339.575129 128.318499 \r\nL 340.185076 124.552064 \r\nL 340.795023 110.360557 \r\nL 341.40497 87.693845 \r\nL 342.014917 81.788036 \r\nL 342.624865 111.933183 \r\nL 343.234812 125.604962 \r\nL 343.844759 133.371355 \r\nL 344.454706 132.044904 \r\nL 345.064653 120.387251 \r\nL 345.6746 121.481117 \r\nL 346.284548 102.673839 \r\nL 347.504442 149.743929 \r\nL 348.114389 162.140605 \r\nL 348.724336 171.316127 \r\nL 349.334283 176.313025 \r\nL 351.164125 168.879186 \r\nL 351.774072 151.607389 \r\nL 352.384019 152.036654 \r\nL 352.993966 152.723896 \r\nL 353.603914 153.687485 \r\nL 354.213861 154.955871 \r\nL 354.823808 157.527469 \r\nL 356.043702 168.597021 \r\nL 356.653649 163.214756 \r\nL 357.263597 145.103846 \r\nL 357.873544 187.616854 \r\nL 358.483491 159.231703 \r\nL 359.093438 142.647489 \r\nL 359.703385 131.220984 \r\nL 360.313332 124.708698 \r\nL 360.92328 187.391042 \r\nL 361.533227 215.465393 \r\nL 362.143174 226.043636 \r\nL 362.753121 229.874489 \r\nL 363.363068 228.112095 \r\nL 363.363068 228.112095 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_17\">\r\n    <!-- model loss -->\r\n    <g transform=\"translate(179.001563 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"370.947266\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"432.128906\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"484.228516\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 59.674375 \r\nL 132.015625 59.674375 \r\nQ 134.015625 59.674375 134.015625 57.674375 \r\nL 134.015625 29.318125 \r\nQ 134.015625 27.318125 132.015625 27.318125 \r\nL 50.78125 27.318125 \r\nQ 48.78125 27.318125 48.78125 29.318125 \r\nL 48.78125 57.674375 \r\nQ 48.78125 59.674375 50.78125 59.674375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 52.78125 35.416562 \r\nL 72.78125 35.416562 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_18\">\r\n     <!-- train -->\r\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 52.78125 50.094687 \r\nL 72.78125 50.094687 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- validation -->\r\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"176.025391\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"239.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"300.78125\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"339.990234\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"367.773438\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"428.955078\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pbdd3afba13\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# 画图\n",
    "PlotOfModel(ACC,VAL_ACC,LOSS,VAL_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_05_end.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2212\n738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(TotalOfTrain)\n",
    "print(TotalOfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "PlotOfModel2(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotOfModel2(history):\n",
    "    # 可视化模型的准确度和损失率\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()  \n",
    "\n",
    "def PlotOfModel(acc,val_acc,loss,val_loss):\n",
    "    # 可视化模型的准确度和损失率\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据提取函数，把图片、标签分开，并且把两种预测结果分开\n",
    "def create_data(DIR,DataOfName):\n",
    "    #创建数据集，兼容五种格式\n",
    "    data=[]\n",
    "    Positive_img_data=[]\n",
    "    Positive_lab_data=[]\n",
    "    Negative_img_data=[]\n",
    "    Negative_lab_data=[]\n",
    "    for i in range(len(DataOfName)):\n",
    "        FileName=DataOfName[i]\n",
    "        path=os.path.join(DIR,FileName)\n",
    "        #如果是图片\n",
    "        if ('png' in FileName or 'jpg'in FileName or 'jfif'in FileName):\n",
    "            create_data_from_img(path,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,DataOfLab[i])\n",
    "        elif ('pdf'in FileName):\n",
    "            create_data_from_pdf(path,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,DataOfLab[i])\n",
    "        elif('docx'in FileName):\n",
    "            create_data_from_docx(path,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,DataOfLab[i])\n",
    "        elif('MOV'in FileName or 'mp4'in FileName):\n",
    "            create_data_from_video(path,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,DataOfLab[i])\n",
    "        elif('zip'in FileName):\n",
    "            create_data_from_zip(path,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,DataOfLab[i])\n",
    "    \n",
    "    return Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data\n",
    "\n",
    "def append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num):\n",
    "    if class_num==0:\n",
    "        Positive_img_data.append(new_array)\n",
    "        Positive_lab_data.append(class_num)\n",
    "    elif class_num==1:\n",
    "        Negative_img_data.append(new_array)\n",
    "        Negative_lab_data.append(class_num)\n",
    "\n",
    "def create_data_from_img(DIR,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,category):\n",
    "    #如果数据是图片，png、jpg、jfif\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    img_array= cv2.imread(DIR) #读取照片\n",
    "    new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "\n",
    "    append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num)\n",
    "    \n",
    "    \n",
    "def create_data_from_pdf(DIR,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,category):\n",
    "    # 在PDF中提取照片信息\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    pic_path = DIR[0:len(DIR)-4] #解压后的路径\n",
    "    # 创建保存图片的文件夹，如果已经存在则不创建\n",
    "    if os.path.exists(pic_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(pic_path)\n",
    "\n",
    "    t0 = time.clock()                          # 生成图片初始时间\n",
    "    checkXO = r\"/Type(?= */XObject)\"           # 使用正则表达式来查找图片\n",
    "    checkIM = r\"/Subtype(?= */Image)\"\n",
    "    doc = fitz.open(DIR)                      # 打开pdf文件\n",
    "    imgcount = 0                               # 图片计数\n",
    "    lenXREF = doc._getXrefLength()             # 获取对象数量长度\n",
    "     \n",
    "\n",
    "    # 遍历每一个对象\n",
    "    for i in range(1, lenXREF):\n",
    "        text = doc._getXrefString(i)            # 定义对象字符串\n",
    "        isXObject = re.search(checkXO, text)    # 使用正则表达式查看是否是对象\n",
    "        isImage = re.search(checkIM, text)      # 使用正则表达式查看是否是图片\n",
    "        if not isXObject or not isImage:        # 如果不是对象也不是图片，则continue\n",
    "            continue\n",
    "        imgcount += 1\n",
    "\n",
    "        pix = fitz.Pixmap(doc, i)               # 生成图像对象\n",
    "        new_name = \"Img{}.png\".format(imgcount) # 生成图片的名称\n",
    "        if pix.n < 5:                           # 如果pix.n<5,可以直接存为PNG\n",
    "            pix.writePNG(os.path.join(pic_path, new_name))\n",
    "        else:                                   # 否则先转换CMYK\n",
    "            pix0 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            pix0.writePNG(os.path.join(pic_path, new_name))\n",
    "            pix0 = None\n",
    "        pix = None                              # 释放资源\n",
    "        t1 = time.clock()                       # 图片完成时间\n",
    "        print(\"run:{}s\".format(t1 - t0))\n",
    "        print(\"Total take {} imges\".format(imgcount))\n",
    "\n",
    "        # 进到文件夹里边读取图像数据\n",
    "        img_array= cv2.imread(os.path.join(pic_path, new_name)) #读取照片\n",
    "        new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "\n",
    "        append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num)\n",
    "        \n",
    "\n",
    "\n",
    "def create_data_from_docx(DIR,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,category):\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    path=DIR\n",
    "    zip_path=DIR[0:len(DIR)-4]+'zip'\n",
    "    tmp_path=DIR[0:len(DIR)-5]+'temp'\n",
    "    store_path=DIR[0:len(DIR)-5]\n",
    "\n",
    "    # 创建保存图片的文件夹，如果已经存在则不创建\n",
    "    if os.path.exists(tmp_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(tmp_path)\n",
    "    if os.path.exists(store_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(store_path)\n",
    "    \n",
    "    # 将docx文件重命名为zip文件\n",
    "    os.rename(path, zip_path)\n",
    "    f = zipfile.ZipFile(zip_path, 'r')# 进行解压\n",
    "    for file in f.namelist():# 将图片提取并保存\n",
    "        f.extract(file, tmp_path)\n",
    "    f.close()# 释放该zip文件\n",
    "\n",
    "    # 将docx文件从zip还原为docx\n",
    "    os.rename(zip_path, path)\n",
    "    # 得到缓存文件夹中图片列表\n",
    "    pic = os.listdir(os.path.join(tmp_path, 'word/media'))\n",
    "\n",
    "    # 将图片复制到最终的文件夹中\n",
    "    for i in pic:\n",
    "        # 根据word的路径生成图片的名称\n",
    "        new_name = path.replace('\\\\', '_')\n",
    "        new_name = new_name.replace(':', '') + '_' + i\n",
    "        shutil.copy(os.path.join(tmp_path + '/word/media', i), os.path.join(store_path, new_name))\n",
    "\n",
    "        # 读取数据\n",
    "        img_array= cv2.imread(os.path.join(store_path, new_name)) #读取照片\n",
    "        new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "\n",
    "        append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num)\n",
    "\n",
    "    # 删除缓冲文件夹中的文件，用以存储下一次的文件\n",
    "    for i in os.listdir(tmp_path):\n",
    "        # 如果是文件夹则删除\n",
    "        if os.path.isdir(os.path.join(tmp_path, i)):\n",
    "            shutil.rmtree(os.path.join(tmp_path, i))\n",
    "\n",
    "\n",
    "def create_data_from_video(DIR,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,category):\n",
    "    # 从视频中读取数据，一帧一帧读取\n",
    "    cap = cv2.VideoCapture(DIR)#打开视频\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    count=0\n",
    "    count_max=24-1\n",
    "    while cap.isOpened():#如果成功打开\n",
    "        rval, image = cap.read()\n",
    "        if rval==True:\n",
    "            if count==count_max:\n",
    "                new_array = cv2.resize(image, (image_size, image_size))  \n",
    "                append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num)\n",
    "                count=0\n",
    "            else:\n",
    "                count+=1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "def create_data_from_zip(DIR,Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,category):\n",
    "    #从压缩包中得到数据\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    with zipfile.ZipFile(DIR, mode='r') as zfile: # 只读方式打开压缩包\n",
    " \n",
    "        for name in zfile.namelist():  # 获取zip文档内所有文件的名称列表\n",
    "            if ('.jpg' not in name) or ('.JPG'not in name):# 仅读取.jpg图片，过滤掉文件夹，及其他非.jpg后缀文件\n",
    "                continue\n",
    "            #print(name)\n",
    "            with zfile.open(name,mode='r') as image_file:\n",
    "                content = image_file.read() # 一次性读入整张图片信息\n",
    "                image = np.asarray(bytearray(content), dtype='uint8')\n",
    "                new_array = cv2.resize(image, (image_size, image_size))  \n",
    "                \n",
    "                append_data(Positive_img_data,Positive_lab_data,Negative_img_data,Negative_lab_data,new_array,class_num)\n",
    "\n",
    "    zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据预处理打包成函数，想放到训练的时候每一次都调用一次，增强数据\n",
    "\n",
    "def TrainData_process(data):\n",
    "    # 对训练数据进行数据增强、预处理\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for features,label in data:\n",
    "        X.append(features)\n",
    "        Y.append(label)\n",
    "\n",
    "    X = np.array(X).reshape(-1, image_size, image_size, 3)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    # 针对训练数据集的预处理，数据增强，减少过拟合\n",
    "    train_image_generator = ImageDataGenerator(\n",
    "        rescale=1./255,         # 此值将在执行前成到整个图像上，把数值归一化\n",
    "        rotation_range=45,      # 随机进行转动的角度\n",
    "        horizontal_flip=True,   # 是否进行随机水平翻转\n",
    "        vertical_flip=True,     # 是否随机竖直翻转\n",
    "        width_shift_range=0.1,  # 随机水平偏移的幅度\n",
    "        height_shift_range=0.1, # 随机竖直偏移的幅度\n",
    "        zoom_range=0.5          # 随机进行放大的幅度，浮点数或者[low，hight]\n",
    "        )\n",
    "    train_data_gen = train_image_generator.flow(X, Y, batch_size = BATCH_SIZE)  # 抽取batch_size数量的数据组合为一个batch\n",
    "    # print()\n",
    "    # print(\"===调用1次===\")\n",
    "    # print()\n",
    "    return train_data_gen\n",
    "\n",
    "\n",
    "def TestData_process(data):\n",
    "    # 对测试数据预处理\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for features,label in data:\n",
    "        X.append(features)\n",
    "        Y.append(label)\n",
    "\n",
    "    X = np.array(X).reshape(-1, image_size, image_size, 3)\n",
    "    Y = np.array(Y)    \n",
    "    # 针对验证数据集，不需数据增强\n",
    "    validation_image_generator = ImageDataGenerator(rescale=1./255)\n",
    "    val_data_gen = validation_image_generator.flow(X, Y, batch_size = BATCH_SIZE) # 抽取batch_size数量的数据组合为一个batch\n",
    "    return val_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(DIR):\n",
    "    #创建数据集，兼容五种格式\n",
    "    data=[]\n",
    "    for i in range(len(DataOfName)):\n",
    "        FileName=DataOfName[i]\n",
    "        path=os.path.join(DIR,FileName)\n",
    "        #如果是图片\n",
    "        if ('png' in FileName or 'jpg'in FileName or 'jfif'in FileName):\n",
    "            create_data_from_img(path,data,DataOfLab[i])\n",
    "        elif ('pdf'in FileName):\n",
    "            create_data_from_pdf(path,data,DataOfLab[i])\n",
    "        elif('docx'in FileName):\n",
    "            create_data_from_docx(path,data,DataOfLab[i])\n",
    "        elif('MOV'in FileName or 'mp4'in FileName):\n",
    "            create_data_from_video(path,data,DataOfLab[i])\n",
    "        elif('zip'in FileName):\n",
    "            create_data_from_zip(path,data,DataOfLab[i])\n",
    "    return data\n",
    "\n",
    "def create_data_from_img(DIR,data,category):\n",
    "    #如果数据是图片，png、jpg、jfif\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    img_array= cv2.imread(DIR) #读取照片\n",
    "    new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "    data.append([new_array,class_num])#进行数据组合\n",
    "    \n",
    "    \n",
    "def create_data_from_pdf(DIR,data,category):\n",
    "    # 在PDF中提取照片信息\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    pic_path = DIR[0:len(DIR)-4] #解压后的路径\n",
    "    # 创建保存图片的文件夹，如果已经存在则不创建\n",
    "    if os.path.exists(pic_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(pic_path)\n",
    "\n",
    "    t0 = time.clock()                          # 生成图片初始时间\n",
    "    checkXO = r\"/Type(?= */XObject)\"           # 使用正则表达式来查找图片\n",
    "    checkIM = r\"/Subtype(?= */Image)\"\n",
    "    doc = fitz.open(DIR)                      # 打开pdf文件\n",
    "    imgcount = 0                               # 图片计数\n",
    "    lenXREF = doc._getXrefLength()             # 获取对象数量长度\n",
    "     \n",
    "\n",
    "    # 遍历每一个对象\n",
    "    for i in range(1, lenXREF):\n",
    "        text = doc._getXrefString(i)            # 定义对象字符串\n",
    "        isXObject = re.search(checkXO, text)    # 使用正则表达式查看是否是对象\n",
    "        isImage = re.search(checkIM, text)      # 使用正则表达式查看是否是图片\n",
    "        if not isXObject or not isImage:        # 如果不是对象也不是图片，则continue\n",
    "            continue\n",
    "        imgcount += 1\n",
    "\n",
    "        pix = fitz.Pixmap(doc, i)               # 生成图像对象\n",
    "        new_name = \"Img{}.png\".format(imgcount) # 生成图片的名称\n",
    "        if pix.n < 5:                           # 如果pix.n<5,可以直接存为PNG\n",
    "            pix.writePNG(os.path.join(pic_path, new_name))\n",
    "        else:                                   # 否则先转换CMYK\n",
    "            pix0 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            pix0.writePNG(os.path.join(pic_path, new_name))\n",
    "            pix0 = None\n",
    "        pix = None                              # 释放资源\n",
    "        t1 = time.clock()                       # 图片完成时间\n",
    "        print(\"run:{}s\".format(t1 - t0))\n",
    "        print(\"Total take {} imges\".format(imgcount))\n",
    "\n",
    "        # 进到文件夹里边读取图像数据\n",
    "        img_array= cv2.imread(os.path.join(pic_path, new_name)) #读取照片\n",
    "        new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "        data.append([new_array,class_num])#进行数据组合\n",
    "\n",
    "    \n",
    "\n",
    "def create_data_from_docx(DIR,data,category):\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    path=DIR\n",
    "    zip_path=DIR[0:len(DIR)-4]+'zip'\n",
    "    tmp_path=DIR[0:len(DIR)-5]+'temp'\n",
    "    store_path=DIR[0:len(DIR)-5]\n",
    "\n",
    "    # 创建保存图片的文件夹，如果已经存在则不创建\n",
    "    if os.path.exists(tmp_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(tmp_path)\n",
    "    if os.path.exists(store_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(store_path)\n",
    "    \n",
    "    # 将docx文件重命名为zip文件\n",
    "    os.rename(path, zip_path)\n",
    "    f = zipfile.ZipFile(zip_path, 'r')# 进行解压\n",
    "    for file in f.namelist():# 将图片提取并保存\n",
    "        f.extract(file, tmp_path)\n",
    "    f.close()# 释放该zip文件\n",
    "\n",
    "    # 将docx文件从zip还原为docx\n",
    "    os.rename(zip_path, path)\n",
    "    # 得到缓存文件夹中图片列表\n",
    "    pic = os.listdir(os.path.join(tmp_path, 'word/media'))\n",
    "\n",
    "    # 将图片复制到最终的文件夹中\n",
    "    for i in pic:\n",
    "        # 根据word的路径生成图片的名称\n",
    "        new_name = path.replace('\\\\', '_')\n",
    "        new_name = new_name.replace(':', '') + '_' + i\n",
    "        shutil.copy(os.path.join(tmp_path + '/word/media', i), os.path.join(store_path, new_name))\n",
    "\n",
    "        # 读取数据\n",
    "        img_array= cv2.imread(os.path.join(store_path, new_name)) #读取照片\n",
    "        new_array= cv2.resize(img_array,(image_size,image_size))#转化大小\n",
    "        data.append([new_array,class_num])#进行数据组合\n",
    "\n",
    "    # 删除缓冲文件夹中的文件，用以存储下一次的文件\n",
    "    for i in os.listdir(tmp_path):\n",
    "        # 如果是文件夹则删除\n",
    "        if os.path.isdir(os.path.join(tmp_path, i)):\n",
    "            shutil.rmtree(os.path.join(tmp_path, i))\n",
    "\n",
    "\n",
    "def create_data_from_video(DIR,data,category):\n",
    "    # 从视频中读取数据，一帧一帧读取\n",
    "    cap = cv2.VideoCapture(DIR)#打开视频\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    while cap.isOpened():#如果成功打开\n",
    "        rval, image = cap.read()\n",
    "        if rval==True:\n",
    "            new_array = cv2.resize(image, (image_size, image_size))  \n",
    "            data.append([new_array,class_num])#进行数据组合\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "def create_data_from_zip(DIR,data,category):\n",
    "    #从压缩包中得到数据\n",
    "    class_num=CATEGORIES.index(category) #读取分类类别，0是Positive ID，1是Negative ID\n",
    "    with zipfile.ZipFile(DIR, mode='r') as zfile: # 只读方式打开压缩包\n",
    " \n",
    "        for name in zfile.namelist():  # 获取zip文档内所有文件的名称列表\n",
    "            if ('.jpg' not in name) or ('.JPG'not in name):# 仅读取.jpg图片，过滤掉文件夹，及其他非.jpg后缀文件\n",
    "                continue\n",
    "            #print(name)\n",
    "            with zfile.open(name,mode='r') as image_file:\n",
    "                content = image_file.read() # 一次性读入整张图片信息\n",
    "                image = np.asarray(bytearray(content), dtype='uint8')\n",
    "                new_array = cv2.resize(image, (image_size, image_size))  \n",
    "                data.append([new_array,class_num])#进行数据组合\n",
    "\n",
    "    zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}